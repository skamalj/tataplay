{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client =  genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "myfile = client.files.upload(file='/home/skamalj/dev/tataplay/separated_audio/htdemucs/media_1_audio/vocals.wav')\n",
    "\n",
    "response = client.models.generate_content(\n",
    "  model='gemini-2.5-flash',\n",
    "  contents=[\n",
    "    \"\"\"You are given an audio clip. \n",
    "    Task:\n",
    "    1. Transcribe the speech in its original language.\n",
    "    2. Translate it into Tamil, but:\n",
    "       - If the original speaker uses words from another language (e.g., English words in Hindi),\n",
    "         preserve those words exactly in the translated text. Do not re-translate them.\n",
    "       - The translation must feel natural and conversational.\n",
    "    3. Assume there are 3 speakers (Speaker 1, Speaker 2, Speaker 3).\n",
    "    4. Perform speaker diarization: identify continuous segments by the same speaker.\n",
    "    5. Return output strictly in valid JSON with this structure:\n",
    "\n",
    "    {\n",
    "      \"speakers_count\": <int>,\n",
    "      \"speak_segments\": [\n",
    "        {\n",
    "          \"speaker\": \"<speaker_id>\",\n",
    "          \"start_time\": \"<float_seconds>\",\n",
    "          \"end_time\": \"<float_seconds>\",\n",
    "          \"original_text\": \"<transcribed_text_in_original_language>\",\n",
    "          \"translated_text\": \"<translated_text_in_targeted_language_with_code_switch_words_preserved>\",\n",
    "          \"ssml\": \"<speak>...</speak>  // built from translated_text only\"\n",
    "        },\n",
    "        ...\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"translation_language\": \"<targeted language\",\n",
    "        \"total_duration\": \"<float_seconds>\",\n",
    "        \"notes\": \"pitch, pauses, tone captured where possible; code-switch words preserved\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "    Rules:\n",
    "    - Each segment must include both original_text and translated_text.\n",
    "    - The translated_text must preserve any words from another language exactly as spoken.\n",
    "      Example: If the speaker says \"वैसे कल आपका स्माइल करना मेरे लिए काफी हिस्टोरिकल था\",\n",
    "      then translated_text could be: \"<translated text.\" (with \"smile\" and \"historical\" kept in English).\n",
    "    - The ssml field must be built from translated_text only.\n",
    "    - Each segment must have its own <speak> block with a <voice> tag unique to the speaker.\n",
    "    - Use <break> and <prosody> to reflect pauses, pitch, and tone.\n",
    "    - Do not include any explanation outside the JSON.\n",
    "    \"\"\",\n",
    "    myfile,\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions (just run that cell)\n",
    "\n",
    "import contextlib\n",
    "import wave\n",
    "from IPython.display import Audio\n",
    "\n",
    "file_index = 0\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def wave_file(filename, channels=1, rate=24000, sample_width=2):\n",
    "    with wave.open(filename, \"wb\") as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(sample_width)\n",
    "        wf.setframerate(rate)\n",
    "        yield wf\n",
    "\n",
    "def play_audio_blob(blob):\n",
    "  global file_index\n",
    "  file_index += 1\n",
    "\n",
    "  fname = f'audio_{file_index}.wav'\n",
    "  with wave_file(fname) as wav:\n",
    "    wav.writeframes(blob.data)\n",
    "\n",
    "  return Audio(fname, autoplay=True)\n",
    "\n",
    "def play_audio(response):\n",
    "    return play_audio_blob(response.candidates[0].content.parts[0].inline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example: Gemini response text\n",
    "text = response.text\n",
    "\n",
    "# Remove leading ```json and trailing ```\n",
    "if text.startswith(\"```json\"):\n",
    "    text = text[len(\"```json\"):].strip()\n",
    "if text.endswith(\"```\"):\n",
    "    text = text[:-3].strip()\n",
    "\n",
    "# Now parse as JSON\n",
    "gemini_data = json.loads(text)\n",
    "\n",
    "# save gemini_data to a file\n",
    "with open('translated_diarized_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(gemini_data, f, indent=2,ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n",
    "   with wave.open(filename, \"wb\") as wf:\n",
    "      wf.setnchannels(channels)\n",
    "      wf.setsampwidth(sample_width)\n",
    "      wf.setframerate(rate)\n",
    "      wf.writeframes(pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "from pathlib import Path\n",
    "\n",
    "def synthesize_segment_tts(client, segment, output_dir=\"translated_segments\", voice_name=\"Puck\"):\n",
    "    \"\"\"\n",
    "    Generate TTS audio for a single segment, keeping duration for lip-sync.\n",
    "\n",
    "    segment: dict with keys\n",
    "        - ssml: SSML string\n",
    "        - start_time: float (seconds)\n",
    "        - end_time: float (seconds)\n",
    "        - speaker: string\n",
    "    client: gemini client\n",
    "    output_dir: where to save the segment audio\n",
    "    voice_name: prebuilt Gemini TTS voice\n",
    "    \"\"\"\n",
    "    import os\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Calculate duration from segment metadata\n",
    "    duration = segment[\"end_time\"] - segment[\"start_time\"]\n",
    "    \n",
    "    # Prepare output file path\n",
    "    seg_index = segment.get(\"index\", 0)\n",
    "\n",
    "    speaker = segment.get(\"speaker\", \"Speaker\")\n",
    "    out_file = Path(output_dir) / f\"{seg_index}_{speaker.replace(' ', '_')}.wav\"\n",
    "\n",
    "    print(f\"Processing segment {seg_index} with duration {duration:.3f} seconds to {out_file}\")\n",
    "    \n",
    "    # Wrap the SSML into the TTS prompt\n",
    "    tts_prompt = f\"\"\"\n",
    "    You are given ssml with timing metadata. \n",
    "    Identify the target language from text. \n",
    "    Must keep original pauses and durations, which is {duration:.3f} seconds, intact for lip-sync. \n",
    "    Generate **audio output**.\n",
    "\n",
    "    Here is the SSML:\n",
    "\n",
    "    {segment['ssml']}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-pro-preview-tts',\n",
    "        contents=tts_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=[\"AUDIO\"],\n",
    "            speech_config=types.SpeechConfig(\n",
    "                voice_config=types.VoiceConfig(\n",
    "                    prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
    "                        voice_name=voice_name,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    print(response.candidates[0])\n",
    "    # Save the returned audio bytes\n",
    "    if response.candidates[0].content:\n",
    "        data = response.candidates[0].content.parts[0].inline_data.data # Gemini returns audio bytes in response.audio\n",
    "        create_wave_file(str(out_file), data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm translated_segments/*.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `gemini_data` is your parsed Gemini JSON with speak_segments\n",
    "for idx, seg in enumerate(gemini_data[\"speak_segments\"]):\n",
    "    seg[\"index\"] = idx\n",
    "    synthesize_segment_tts(client, seg, voice_name=\"Puck\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Paths\n",
    "json_path = \"translated_diarized_data.json\"\n",
    "tts_dir = Path(\"translated_segments\")          # TTS-generated segments\n",
    "adjusted_dir = Path(\"adjusted_segments\")      # Save adjusted segments\n",
    "adjusted_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load JSON\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "segments = data[\"speak_segments\"]\n",
    "\n",
    "\n",
    "for idx, seg in enumerate(segments):\n",
    "    speaker = seg.get(\"speaker\", \"Speaker\")\n",
    "    \n",
    "    tts_file = tts_dir / f\"{idx}_{speaker.replace(' ', '_')}.wav\"\n",
    "    if not tts_file.exists():\n",
    "        print(f\"Missing file: {tts_file}\")\n",
    "        continue\n",
    "\n",
    "    # Target duration from JSON\n",
    "    target_duration = seg[\"end_time\"] - seg[\"start_time\"]  # in seconds\n",
    "\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(tts_file, sr=None)\n",
    "    current_duration = len(y) / sr\n",
    "\n",
    "    # Skip if already close enough\n",
    "    if abs(current_duration - target_duration) < 0.01:\n",
    "        adjusted_audio = y\n",
    "    else:\n",
    "        rate = current_duration / target_duration\n",
    "        adjusted_audio = librosa.effects.time_stretch(y, rate=rate)\n",
    "\n",
    "    # Save adjusted segment\n",
    "    out_file = adjusted_dir / f\"adjusted_seg_{idx}_{seg['speaker']}.wav\"\n",
    "    sf.write(out_file, adjusted_audio, sr)\n",
    "    print(f\"Saved adjusted segment: {out_file} (target: {target_duration:.2f}s, actual: {current_duration:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to OpenVoice Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------- Load Gemini segments -----------\n",
    "with open(\"translated_diarized_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gemini_data = json.load(f)\n",
    "\n",
    "segments = gemini_data.get(\"speak_segments\", [])\n",
    "\n",
    "print(f\"Loaded {len(segments)} segments from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from openvoice.api import ToneColorConverter\n",
    "\n",
    "ckpt_converter = 'checkpoints_v2/converter'\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# OpenVoice se_extractor import (make sure OpenVoice is on PYTHONPATH)\n",
    "from openvoice import se_extractor\n",
    "\n",
    "def _sanitize_speaker(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-]\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def build_reference_embeddings_from_diarization(\n",
    "    original_audio_path: str,\n",
    "    gemini_data: dict,\n",
    "    out_dir: str = \"reference_segments\",\n",
    "    join_silence_ms: int = 100,\n",
    "    sample_rate: int = 24000,\n",
    "    min_total_duration_sec: float = 2.5,\n",
    "    tone_color_converter=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate all segments for each speaker (in chronological order),\n",
    "    export a single WAV reference per speaker, then create + save OpenVoice embeddings.\n",
    "\n",
    "    Returns:\n",
    "      dict mapping speaker -> {\"ref_wav\": str, \"embedding\": str, \"duration_s\": float}\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load full original audio once (pydub uses ffmpeg behind the scenes)\n",
    "    audio = AudioSegment.from_file(str(original_audio_path))\n",
    "    full_duration_ms = len(audio)\n",
    "    print(f\"[info] Loaded original audio '{original_audio_path}' ({full_duration_ms/1000:.2f}s)\")\n",
    "\n",
    "    # Group segments by speaker (keeps order)\n",
    "    speaker_segments = defaultdict(list)\n",
    "    for seg in gemini_data.get(\"speak_segments\", []):\n",
    "        try:\n",
    "            start_ms = int(round(float(seg[\"start_time\"]) * 1000))\n",
    "            end_ms   = int(round(float(seg[\"end_time\"]) * 1000))\n",
    "        except Exception as e:\n",
    "            print(f\"[warning] skipping segment with bad times: {seg} -> {e}\")\n",
    "            continue\n",
    "        if end_ms <= start_ms:\n",
    "            print(f\"[warning] skipping zero/negative-length segment: start={start_ms} end={end_ms}\")\n",
    "            continue\n",
    "        # clamp within audio\n",
    "        start_ms = max(0, min(start_ms, full_duration_ms))\n",
    "        end_ms   = max(0, min(end_ms, full_duration_ms))\n",
    "        dur_ms = end_ms - start_ms\n",
    "        speaker = seg.get(\"speaker\", \"unknown\")\n",
    "        speaker_segments[speaker].append({\"start_ms\": start_ms, \"end_ms\": end_ms, \"duration_ms\": dur_ms})\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for speaker, segs in speaker_segments.items():\n",
    "        # sort by start time to keep natural order\n",
    "        segs_sorted = sorted(segs, key=lambda x: x[\"start_ms\"])\n",
    "        concat = AudioSegment.silent(duration=0, frame_rate=sample_rate)\n",
    "\n",
    "        total_ms = 0\n",
    "        for s in segs_sorted:\n",
    "            start_ms = s[\"start_ms\"]\n",
    "            end_ms = s[\"end_ms\"]\n",
    "            # slice original audio\n",
    "            clip = audio[start_ms:end_ms]\n",
    "            # append clip\n",
    "            concat += clip\n",
    "            total_ms += len(clip)\n",
    "            # add tiny silence between clips to avoid gluing words (optional)\n",
    "            concat += AudioSegment.silent(duration=join_silence_ms, frame_rate=sample_rate)\n",
    "\n",
    "        total_s = total_ms / 1000.0\n",
    "\n",
    "        # Warn if too short\n",
    "        if total_s < min_total_duration_sec:\n",
    "            warnings.warn(\n",
    "                f\"[warn] total concatenated duration for speaker '{speaker}' is short ({total_s:.2f}s). \"\n",
    "                \"Embeddings may be poor. Consider merging more segments or increasing min_total_duration_sec.\"\n",
    "            )\n",
    "\n",
    "        # Trim trailing silence\n",
    "        # If concat is longer than join silence at end, remove last join_silence_ms\n",
    "        if len(concat) >= join_silence_ms:\n",
    "            concat = concat[:-join_silence_ms]\n",
    "\n",
    "        # Normalize export settings\n",
    "        concat = concat.set_frame_rate(sample_rate).set_channels(1)\n",
    "\n",
    "        # Save reference wav\n",
    "        speaker_safe = _sanitize_speaker(speaker)\n",
    "        ref_path = out_dir / f\"ref_{speaker_safe}.wav\"\n",
    "        concat.export(str(ref_path), format=\"wav\")\n",
    "        print(f\"[ok] Saved ref audio for '{speaker}' -> {ref_path} ({total_s:.2f}s)\")\n",
    "\n",
    "        # Create embedding via se_extractor\n",
    "        try:\n",
    "            # se_extractor.get_se may accept different signatures; attempt with tone_color_converter first\n",
    "            if tone_color_converter is not None:\n",
    "                source_se, audio_name = se_extractor.get_se(str(ref_path), tone_color_converter, vad=True)\n",
    "            else:\n",
    "                # fallback signature\n",
    "                source_se, audio_name = se_extractor.get_se(str(ref_path), vad=True)\n",
    "        except TypeError:\n",
    "            # try alternate call (some builds expect only path)\n",
    "            source_se, audio_name = se_extractor.get_se(str(ref_path))\n",
    "\n",
    "        emb_path = out_dir / f\"{speaker_safe}_embedding.pt\"\n",
    "        torch.save(source_se, str(emb_path))\n",
    "        print(f\"[ok] Saved embedding for '{speaker}' -> {emb_path}\")\n",
    "\n",
    "        results[speaker] = {\n",
    "            \"ref_wav\": str(ref_path),\n",
    "            \"embedding\": str(emb_path),\n",
    "            \"duration_s\": total_s,\n",
    "            \"num_segments\": len(segs_sorted),\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini_data = json.load(open(\"translated_diarized_data.json\", encoding=\"utf-8\"))\n",
    "# original_audio = \"original_audio.mp3\"\n",
    "\n",
    "ref_map = build_reference_embeddings_from_diarization(\n",
    "    original_audio_path=\"media_1_audio.mp3\",\n",
    "    gemini_data=gemini_data,\n",
    "    out_dir=\"reference_segments\",\n",
    "    join_silence_ms=100,\n",
    "    sample_rate=24000,\n",
    "    min_total_duration_sec=5.0,\n",
    "    tone_color_converter=tone_color_converter  # or None\n",
    ")\n",
    "\n",
    "print(ref_map)\n",
    "# -> {'Speaker 1': {'ref_wav': 'reference_segments/ref_Speaker_1.wav', 'embedding': 'reference_segments/Speaker_1_embedding.pt', ...}, ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def build_src_embedding_from_tts(tts_dir: str, gemini_data: dict, tone_color_converter=None, max_len_sec=10):\n",
    "    \"\"\"\n",
    "    Combine TTS segments (up to max_len_sec) to create one source embedding.\n",
    "    \"\"\"\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "    total_ms = 0\n",
    "\n",
    "    for idx, seg in enumerate(gemini_data[\"speak_segments\"]):\n",
    "        tts_path = Path(tts_dir) / f\"{idx}_{seg['speaker']}.wav\"\n",
    "        if not tts_path.exists():\n",
    "            continue\n",
    "\n",
    "        seg_audio = AudioSegment.from_wav(tts_path)\n",
    "        combined += seg_audio\n",
    "        total_ms += len(seg_audio)\n",
    "\n",
    "        if total_ms >= max_len_sec * 1000:\n",
    "            break\n",
    "\n",
    "    # Save temporary combined TTS clip\n",
    "    tmp_path = Path(tts_dir) / \"tts_src_reference.wav\"\n",
    "    combined.export(tmp_path, format=\"wav\")\n",
    "\n",
    "    # Extract embedding once\n",
    "    if tone_color_converter is not None:\n",
    "        src_se, _ = se_extractor.get_se(str(tmp_path), tone_color_converter, vad=True)\n",
    "    else:\n",
    "        src_se, _ = se_extractor.get_se(str(tmp_path), vad=True)\n",
    "\n",
    "    print(f\"[ok] Source embedding created from {total_ms/1000:.1f}s of TTS audio -> {tmp_path}\")\n",
    "    return src_se, str(tmp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build **single src embedding** from TTS segments\n",
    "src_se, tts_ref_path = build_src_embedding_from_tts(\n",
    "    tts_dir=\"translated_segments\",\n",
    "    gemini_data=gemini_data,\n",
    "    tone_color_converter=tone_color_converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tts_segments_with_refs(tts_dir, gemini_data, ref_map, out_dir, src_se, tone_color_converter):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "    for idx, seg in enumerate(gemini_data.get(\"speak_segments\", [])):\n",
    "        speaker = seg[\"speaker\"]\n",
    "\n",
    "        tts_wav = Path(tts_dir) / f\"{idx}_{speaker}.wav\"\n",
    "        if not tts_wav.exists():\n",
    "            print(f\"[skip] No TTS audio for seg {idx}\")\n",
    "            continue\n",
    "\n",
    "        if speaker not in ref_map:\n",
    "            print(f\"[skip] No reference for {speaker}\")\n",
    "            continue\n",
    "\n",
    "        tgt_se = torch.load(ref_map[speaker][\"embedding\"])\n",
    "        out_wav = out_dir / f\"converted_seg_{idx}_{speaker}.wav\"\n",
    "\n",
    "        tone_color_converter.convert(\n",
    "            audio_src_path=str(tts_wav),\n",
    "            src_se=src_se,   # <-- reuse the same source embedding\n",
    "            tgt_se=tgt_se,\n",
    "            output_path=str(out_wav),\n",
    "            message=\"@MyShell\",\n",
    "        )\n",
    "        print(f\"[ok] Converted seg {idx} ({speaker}) -> {out_wav}\")\n",
    "        results[idx] = str(out_wav)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted = convert_tts_segments_with_refs(\n",
    "    tts_dir=\"translated_segments\",\n",
    "    gemini_data=gemini_data,\n",
    "    ref_map=ref_map,\n",
    "    out_dir=\"converted_segments\",\n",
    "    src_se=src_se,\n",
    "    tone_color_converter=tone_color_converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def mix_segments_with_background(gemini_json_path, converted_dir, background_audio_path, output_path):\n",
    "    # Load segment metadata\n",
    "    with open(gemini_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gemini_data = json.load(f)\n",
    "\n",
    "    speak_segments = gemini_data[\"speak_segments\"]\n",
    "\n",
    "    # Load background audio\n",
    "    bg_audio = AudioSegment.from_file(background_audio_path, format=\"mp3\")\n",
    "    \n",
    "    # Overlay each converted segment\n",
    "    for idx, seg in enumerate(speak_segments):\n",
    "        seg_file = Path(converted_dir) / f\"converted_seg_{idx}_{seg['speaker']}.wav\"\n",
    "        if not seg_file.exists():\n",
    "            print(f\"Skipping missing segment: {seg_file}\")\n",
    "            continue\n",
    "        \n",
    "        segment_audio = AudioSegment.from_file(seg_file, format=\"wav\")\n",
    "        start_ms = int(seg[\"start_time\"] * 1000)\n",
    "        bg_audio = bg_audio.overlay(segment_audio, position=start_ms)\n",
    "        print(f\"Overlayed segment {idx} ({seg['speaker']}) at {start_ms} ms\")\n",
    "\n",
    "    # Export final mix\n",
    "    bg_audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Final audio mix saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "mix_segments_with_background(\n",
    "    gemini_json_path=\"translated_diarized_data.json\",\n",
    "    converted_dir=\"converted_segments\",\n",
    "    background_audio_path=\"separated_audio/htdemucs/media_1_audio/no_vocals.wav\",\n",
    "    output_path=\"final_mix.wav\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Load video and audio clips\n",
    "video = VideoFileClip(\"media_1_video.mp4\")\n",
    "audio = AudioFileClip(\"final_mix.wav\")\n",
    "\n",
    "# Set the new audio to the video\n",
    "final_video = video.with_audio(audio)\n",
    "\n",
    "# Export the combined file\n",
    "final_video.write_videofile(\"output_video_with_audio.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tataplay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
