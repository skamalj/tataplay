{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course, here is a breakdown of the speakers in the audio clip.\n",
      "\n",
      "### Speaker Identification\n",
      "\n",
      "1.  **Gita** - **Female, Expressive and Questioning Tone:**\n",
      "    Gita speaks with a clear, mid-to-high-pitched voice. Her tone is expressive and inquisitive, speaking at a moderate pace. The recording is high-quality with a close, intimate sound.\n",
      "\n",
      "2.  **Gopal** - **Male, Calm and Measured Tone:**\n",
      "    Gopal has a calm, mid-pitched male voice. He speaks in a measured and slightly charming tone at a moderate pace. The audio is captured clearly in a high-quality, close-sounding environment.\n",
      "\n",
      "3.  **Rohan** - **Male Narrator, Upbeat and Rhythmic:**\n",
      "    Rohan speaks in a classic, upbeat narrator's voice with a mid-to-low pitch. His delivery is fast and rhythmic, designed to be memorable for the advertisement's tagline. The recording quality is excellent.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client =  genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "myfile = client.files.upload(file='/home/skamalj/dev/tataplay/separated_audio/htdemucs/media_1_audio/vocals.wav')\n",
    "\n",
    "response = client.models.generate_content(\n",
    "  model='gemini-2.5-pro',\n",
    "  contents=[\n",
    "    \"\"\"You are given an audio clip. \n",
    "    Task:\n",
    "    1. Identify the speakers.\n",
    "    2. Give them indian names basis gender\n",
    "    3. Descrive their speaking style\n",
    "\n",
    "    Stye description examples:\n",
    "    Aditi - Slightly High-Pitched, Expressive Tone:\n",
    "    \"Aditi speaks with a slightly higher pitch in a close-sounding environment. Her voice is clear, with subtle emotional depth and a normal pace, all captured in high-quality recording.\"\n",
    "\n",
    "    Sita - Rapid, Slightly Monotone:\n",
    "    \"Sita speaks at a fast pace with a slightly low-pitched voice, captured clearly in a close-sounding environment with excellent recording quality.\"\n",
    "\n",
    "    Tapan - Male, Moderate Pace, Slightly Monotone:\n",
    "    \"Tapan speaks at a moderate pace with a slightly monotone tone. The recording is clear, with a close sound and only minimal ambient noise.\"\n",
    "    \"\"\",\n",
    "    myfile,\n",
    "  ]\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client =  genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "myfile = client.files.upload(file='/home/skamalj/dev/tataplay/separated_audio/htdemucs/media_1_audio/vocals.wav')\n",
    "\n",
    "response = client.models.generate_content(\n",
    "  model='gemini-2.5-pro',\n",
    "  contents=[\n",
    "    \"\"\"You are given an audio clip. \n",
    "    Task:\n",
    "    1. Transcribe the speech in its original language.\n",
    "    2. Translate it into English, but:\n",
    "       - If the original speaker uses words from another language (e.g., English words in Hindi),\n",
    "         preserve those words exactly in the translated text. Do not re-translate them.\n",
    "       - The translation must feel natural and conversational.\n",
    "    3. Assume there are 3 speakers (Speaker 1, Speaker 2, Speaker 3).\n",
    "    4. Perform speaker diarization: identify continuous segments by the same speaker and merge them.\n",
    "    5. **Break the segments only when speaker changes or there is a significant pause (more than 2 second)**.\n",
    "    6. Capture pitch, pauses, and tone in the SSML output where possible.\n",
    "    7. Use a unique voice for each speaker in the SSML output.\n",
    "    5. Return output strictly in valid JSON with this structure:\n",
    "\n",
    "    {\n",
    "      \"speakers_count\": <int>,\n",
    "      \"speak_segments\": [\n",
    "        {\n",
    "          \"speaker\": \"<speaker_id>\",\n",
    "          \"start_time\": \"<float_seconds>\",\n",
    "          \"end_time\": \"<float_seconds>\",\n",
    "          \"original_text\": \"<transcribed_text_in_original_language>\",\n",
    "          \"translated_text\": \"<translated_text_in_targeted_language_with_code_switch_words_preserved>\",\n",
    "          \"ssml\": \"<speak>...</speak>  // built from translated_text only\"\n",
    "        },\n",
    "        ...\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"translation_language\": \"<targeted language\",\n",
    "        \"total_duration\": \"<float_seconds>\",\n",
    "        \"notes\": \"pitch, pauses, tone captured where possible; code-switch words preserved\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "    Rules:\n",
    "    - Each segment must include both original_text and translated_text.\n",
    "    - The translated_text must preserve any words from another language exactly as spoken.\n",
    "      Example: If the speaker says \"वैसे कल आपका स्माइल करना मेरे लिए काफी हिस्टोरिकल था\",\n",
    "      then translated_text could be: \"<translated text.\" (with \"smile\" and \"historical\" kept in English).\n",
    "    - The ssml field must be built from translated_text only.\n",
    "    - Each segment must have its own <speak> block with a <voice> tag unique to the speaker.\n",
    "    - Use <break> and <prosody> to reflect pauses, pitch, and tone.\n",
    "    - Do not include any explanation outside the JSON.\n",
    "    \"\"\",\n",
    "    myfile,\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions (just run that cell)\n",
    "\n",
    "import contextlib\n",
    "import wave\n",
    "from IPython.display import Audio\n",
    "\n",
    "file_index = 0\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def wave_file(filename, channels=1, rate=24000, sample_width=2):\n",
    "    with wave.open(filename, \"wb\") as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(sample_width)\n",
    "        wf.setframerate(rate)\n",
    "        yield wf\n",
    "\n",
    "def play_audio_blob(blob):\n",
    "  global file_index\n",
    "  file_index += 1\n",
    "\n",
    "  fname = f'audio_{file_index}.wav'\n",
    "  with wave_file(fname) as wav:\n",
    "    wav.writeframes(blob.data)\n",
    "\n",
    "  return Audio(fname, autoplay=True)\n",
    "\n",
    "def play_audio(response):\n",
    "    return play_audio_blob(response.candidates[0].content.parts[0].inline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example: Gemini response text\n",
    "text = response.text\n",
    "\n",
    "# Remove leading ```json and trailing ```\n",
    "if text.startswith(\"```json\"):\n",
    "    text = text[len(\"```json\"):].strip()\n",
    "if text.endswith(\"```\"):\n",
    "    text = text[:-3].strip()\n",
    "\n",
    "# Now parse as JSON\n",
    "gemini_data = json.loads(text)\n",
    "\n",
    "# save gemini_data to a file\n",
    "with open('translated_diarized_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(gemini_data, f, indent=2,ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n",
    "   with wave.open(filename, \"wb\") as wf:\n",
    "      wf.setnchannels(channels)\n",
    "      wf.setsampwidth(sample_width)\n",
    "      wf.setframerate(rate)\n",
    "      wf.writeframes(pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "from pathlib import Path\n",
    "\n",
    "def synthesize_segment_tts(client, segment, output_dir=\"translated_segments\", voice_name=\"Puck\"):\n",
    "    \"\"\"\n",
    "    Generate TTS audio for a single segment, keeping duration for lip-sync.\n",
    "\n",
    "    segment: dict with keys\n",
    "        - ssml: SSML string\n",
    "        - start_time: float (seconds)\n",
    "        - end_time: float (seconds)\n",
    "        - speaker: string\n",
    "    client: gemini client\n",
    "    output_dir: where to save the segment audio\n",
    "    voice_name: prebuilt Gemini TTS voice\n",
    "    \"\"\"\n",
    "    import os\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Calculate duration from segment metadata\n",
    "    duration = float(segment[\"end_time\"])  - float(segment[\"start_time\"])\n",
    "    \n",
    "    # Prepare output file path\n",
    "    seg_index = segment.get(\"index\", 0)\n",
    "\n",
    "    speaker = segment.get(\"speaker\", \"Speaker\")\n",
    "    out_file = Path(output_dir) / f\"{seg_index}_{speaker.replace(' ', '_')}.wav\"\n",
    "\n",
    "    print(f\"Processing segment {seg_index} with duration {duration:.3f} seconds to {out_file}\")\n",
    "    print(segment['translated_text'])\n",
    "    # Wrap the SSML into the TTS prompt\n",
    "    tts_prompt = f\"\"\"\n",
    "    You are given SSML with timing metadata. \n",
    "    Identify the target language from text. If language is english ensure that accent is **Indian English**.\n",
    "    Must keep original pauses and restrict **durations to {duration:.3f} seconds**. \n",
    "    Generate **audio output**.\n",
    "\n",
    "    Here is the SSML:\n",
    "\n",
    "    {segment['ssml']}\n",
    "    \"\"\"\n",
    "    #gemini-2.5-flash-preview-tts\n",
    "    #emini-2.5-pro-preview-tts\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash-preview-tts',\n",
    "        contents=tts_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=[\"AUDIO\"],\n",
    "            speech_config=types.SpeechConfig(\n",
    "                voice_config=types.VoiceConfig(\n",
    "                    prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
    "                        voice_name=voice_name,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    print(response.candidates[0])\n",
    "    # Save the returned audio bytes\n",
    "    if response.candidates[0].content:\n",
    "        data = response.candidates[0].content.parts[0].inline_data.data # Gemini returns audio bytes in response.audio\n",
    "        create_wave_file(str(out_file), data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm translated_segments/*.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block is only for testing. Run for any single segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing segment 3 with duration 4.160 seconds to translated_segments/3_Speaker_2.wav\n",
      "By the way, you smiling yesterday was quite historical for me.\n",
      "content=Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      inline_data=Blob(\n",
      "        data=b'\\xfc\\xff\\xfe\\xff\\x0c\\x00\\x10\\x00\\x03\\x00\\n\\x00\\x19\\x00*\\x009\\x00/\\x00\\x19\\x00\\xed\\xff\\xe6\\xff\\xdb\\xff\\xde\\xff\\x02\\x00\\r\\x00\\x11\\x00\\x10\\x00\\xf1\\xff\\xe2\\xff\\xd7\\xff\\xcf\\xff\\xd4\\xff\\xf7\\xff\\x13\\x00\\x0e\\x00\\xf0\\xff\\xf5\\xff\\n\\x00\\xeb\\xff\\xda\\xff\\xdb\\xff\\xee\\xff\\x0b\\x00\\x1a\\x00\\x10\\x00\\xf1\\xff\\xeb\\xff\\xeb\\xff\\xf0\\xff\\xf3\\xff\\xfc\\xff\\x05\\x00\\x0c\\x00\\t\\x00\\xff\\xff\\xff\\xff\\xfd...',\n",
      "        mime_type='audio/L16;codec=pcm;rate=24000'\n",
      "      )\n",
      "    ),\n",
      "  ],\n",
      "  role='model'\n",
      ") citation_metadata=None finish_message=None token_count=None finish_reason=<FinishReason.STOP: 'STOP'> url_context_metadata=None avg_logprobs=None grounding_metadata=None index=0 logprobs_result=None safety_ratings=None\n"
     ]
    }
   ],
   "source": [
    "n = 3  # change this index as needed\n",
    "seg = gemini_data[\"speak_segments\"][n]\n",
    "seg[\"index\"] = n\n",
    "\n",
    "synthesize_segment_tts(client, seg, voice_name=\"Puck\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "client =  genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing segment 0 with duration 9.880 seconds to translated_segments/0_Speaker_1.wav\n",
      "Gopal ji, what were you teaching in the history class today? You were saying that Tata Sky has now become Tata Play. What kind of a history class is this?\n",
      "content=None citation_metadata=None finish_message=None token_count=None finish_reason=<FinishReason.OTHER: 'OTHER'> url_context_metadata=None avg_logprobs=None grounding_metadata=None index=0 logprobs_result=None safety_ratings=None\n",
      "Processing segment 1 with duration 5.790 seconds to translated_segments/1_Speaker_2.wav\n",
      "Geeta ji, Tata Sky becoming Tata Play is a very historical moment. That's why I was teaching it.\n",
      "content=Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      inline_data=Blob(\n",
      "        data=b'\\xef\\xff\\xea\\xff\\xec\\xff\\xe5\\xff\\xe6\\xff\\xf0\\xff\\x06\\x00 \\x001\\x008\\x00,\\x00\\x16\\x00\\x04\\x00\\xf5\\xff\\xef\\xff\\xea\\xff\\xe9\\xff\\xe6\\xff\\xf0\\xff\\x01\\x00\\x0e\\x00\\r\\x00\\x06\\x00\\x01\\x00\\x01\\x00\\x0c\\x00\\x11\\x00\\x0b\\x00\\xff\\xff\\xf1\\xff\\xe9\\xff\\xf0\\xff\\xf9\\xff\\x01\\x00\\x04\\x00\\x0b\\x00\\x10\\x00\\x0e\\x00\\t\\x00\\x00\\x00\\xf8\\xff\\xef\\xff\\xf1\\xff\\xf2\\xff\\xf9\\xff\\x04\\x00\\x08\\x00\\r\\x00\\x0c...',\n",
      "        mime_type='audio/L16;codec=pcm;rate=24000'\n",
      "      )\n",
      "    ),\n",
      "  ],\n",
      "  role='model'\n",
      ") citation_metadata=None finish_message=None token_count=None finish_reason=<FinishReason.STOP: 'STOP'> url_context_metadata=None avg_logprobs=None grounding_metadata=None index=0 logprobs_result=None safety_ratings=None\n",
      "Processing segment 2 with duration 2.560 seconds to translated_segments/2_Speaker_1.wav\n",
      "Oh really? So what are the other historical moments?\n",
      "content=Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      inline_data=Blob(\n",
      "        data=b'\\xfc\\xff\\xf9\\xff\\xfa\\xff\\xfd\\xff\\x07\\x00\\x06\\x00\\t\\x00\\x12\\x00\\x0f\\x00\\t\\x00\\x04\\x00\\xee\\xff\\xe2\\xff\\xe7\\xff\\xf0\\xff\\xf1\\xff\\xed\\xff\\xe8\\xff\\xd6\\xff\\xc4\\xff\\xbf\\xff\\xbf\\xff\\xd6\\xff\\xf2\\xff\\x0c\\x00\\x18\\x00\\x0f\\x00\\x08\\x00\\xf9\\xff\\xf8\\xff\\x06\\x00\\x03\\x00\\xfc\\xff\\xfd\\xff\\xfa\\xff\\xfc\\xff\\x0c\\x00\\x12\\x00\\x0b\\x00\\x02\\x00\\x03\\x00\\xf0\\xff\\xe4\\xff\\xdd\\xff\\xde\\xff\\xeb\\xff\\t\\x00\\x18\\x00\\x1e...',\n",
      "        mime_type='audio/L16;codec=pcm;rate=24000'\n",
      "      )\n",
      "    ),\n",
      "  ],\n",
      "  role='model'\n",
      ") citation_metadata=None finish_message=None token_count=None finish_reason=<FinishReason.STOP: 'STOP'> url_context_metadata=None avg_logprobs=None grounding_metadata=None index=0 logprobs_result=None safety_ratings=None\n",
      "Processing segment 3 with duration 4.160 seconds to translated_segments/3_Speaker_2.wav\n",
      "By the way, you smiling yesterday was quite historical for me.\n",
      "content=None citation_metadata=None finish_message=None token_count=None finish_reason=<FinishReason.OTHER: 'OTHER'> url_context_metadata=None avg_logprobs=None grounding_metadata=None index=0 logprobs_result=None safety_ratings=None\n",
      "Processing segment 4 with duration 5.460 seconds to translated_segments/4_Speaker_3.wav\n",
      "Now Tata Sky is Tata Play. The name was changed because entertainment is even more Jinga lala.\n",
      "content=Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      inline_data=Blob(\n",
      "        data=b'\\xf2\\xff\\xf1\\xff\\xf0\\xff\\xee\\xff\\xf0\\xff\\xf3\\xff\\xf7\\xff\\xff\\xff\\n\\x00\\x0e\\x00\\x03\\x00\\xf1\\xff\\xe9\\xff\\xe5\\xff\\xe7\\xff\\xed\\xff\\xed\\xff\\xe5\\xff\\xe7\\xff\\xea\\xff\\xec\\xff\\xec\\xff\\xf4\\xff\\xfc\\xff\\x04\\x00\\t\\x00\\x03\\x00\\xf3\\xff\\xe5\\xff\\xde\\xff\\xe0\\xff\\xe5\\xff\\xf0\\xff\\xfb\\xff\\xfc\\xff\\x00\\x00\\x04\\x00\\xfc\\xff\\xf7\\xff\\xed\\xff\\xe8\\xff\\xec\\xff\\xf8\\xff\\x02\\x00\\x03\\x00\\xf9\\xff\\xf3\\xff\\xf9\\xff\\x03...',\n",
      "        mime_type='audio/L16;codec=pcm;rate=24000'\n",
      "      )\n",
      "    ),\n",
      "  ],\n",
      "  role='model'\n",
      ") citation_metadata=None finish_message=None token_count=None finish_reason=<FinishReason.STOP: 'STOP'> url_context_metadata=None avg_logprobs=None grounding_metadata=None index=0 logprobs_result=None safety_ratings=None\n"
     ]
    }
   ],
   "source": [
    "# Assume `gemini_data` is your parsed Gemini JSON with speak_segments\n",
    "for idx, seg in enumerate(gemini_data[\"speak_segments\"]):\n",
    "    seg[\"index\"] = idx\n",
    "    synthesize_segment_tts(client, seg, voice_name=\"Puck\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to stretch a audio for segments. WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "from audiostretchy.stretch import stretch_audio\n",
    "\n",
    "def remove_silence_from_end(audio_path, output_path, silence_thresh=-50, chunk_size=10):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    silence_ranges = detect_silence(audio, min_silence_len=chunk_size, silence_thresh=silence_thresh)\n",
    "\n",
    "    # Default end_trim is full length of audio\n",
    "    end_trim = len(audio)\n",
    "\n",
    "    # Check if last silence segment is at the end of audio\n",
    "    if silence_ranges and silence_ranges[-1][1] == len(audio):\n",
    "        # Trim silence at the end\n",
    "        end_trim = silence_ranges[-1][0]\n",
    "\n",
    "    # Keep audio from start to end_trim (remove silence only at end)\n",
    "    trimmed_audio = audio[:end_trim]\n",
    "    trimmed_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "# Usage example:\n",
    "#remove_silence_from_end(\"translated_segments/1_Speaker_2_compress.wav\", \"translated_segments/trimmed_end_only.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved adjusted segment: adjusted_segments/adjusted_seg_0_Speaker_1.wav (target: 9.88s, actual: 39.93s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_1_Speaker_2.wav (target: 5.79s, actual: 34.33s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_2_Speaker_1.wav (target: 2.56s, actual: 4.73s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_3_Speaker_2.wav (target: 4.16s, actual: 8.37s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_4_Speaker_3.wav (target: 5.46s, actual: 8.33s)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "from audiostretchy.stretch import stretch_audio\n",
    "\n",
    "# Paths\n",
    "json_path = \"translated_diarized_data.json\"\n",
    "tts_dir = Path(\"translated_segments\")          # TTS-generated segments\n",
    "adjusted_dir = Path(\"adjusted_segments\")      # Save adjusted segments\n",
    "adjusted_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load JSON\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "segments = data[\"speak_segments\"]\n",
    "\n",
    "\n",
    "for idx, seg in enumerate(segments):\n",
    "    speaker = seg.get(\"speaker\", \"Speaker\")\n",
    "    \n",
    "    tts_file = tts_dir / f\"{idx}_{speaker.replace(' ', '_')}.wav\"\n",
    "    if not tts_file.exists():\n",
    "        print(f\"Missing file: {tts_file}\")\n",
    "        continue\n",
    "\n",
    "    # Target duration from JSON\n",
    "    target_duration = float(seg[\"end_time\"]) - float(seg[\"start_time\"])  # in seconds\n",
    "\n",
    "   # Load your audio file\n",
    "    audio = AudioSegment.from_file(tts_file)\n",
    "\n",
    "    # Get the duration in milliseconds\n",
    "    current_duration = audio.duration_seconds\n",
    "\n",
    "    # Save adjusted segment\n",
    "    out_file = adjusted_dir / f\"adjusted_seg_{idx}_{seg['speaker'].replace(' ', '_')}.wav\"\n",
    "    out_file_stretched = adjusted_dir / f\"adjusted_seg_{idx}_{seg['speaker'].replace(' ', '_')}_stretched.wav\"\n",
    "\n",
    "    stretch_factor = target_duration / current_duration\n",
    "    adjusted_audio = stretch_audio(tts_file, out_file, ratio=stretch_factor)\n",
    "    remove_silence_from_end(out_file, out_file_stretched)\n",
    "\n",
    "    print(f\"Saved adjusted segment: {out_file} (target: {target_duration:.2f}s, actual: {current_duration:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to OpenVoice Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 segments from file\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------- Load Gemini segments -----------\n",
    "with open(\"translated_diarized_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gemini_data = json.load(f)\n",
    "\n",
    "segments = gemini_data.get(\"speak_segments\", [])\n",
    "\n",
    "print(f\"Loaded {len(segments)} segments from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skamalj/miniconda3/envs/openvoice/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/skamalj/miniconda3/envs/openvoice/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints_v2/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from openvoice.api import ToneColorConverter\n",
    "\n",
    "ckpt_converter = 'checkpoints_v2/converter'\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# OpenVoice se_extractor import (make sure OpenVoice is on PYTHONPATH)\n",
    "from openvoice import se_extractor\n",
    "\n",
    "def _sanitize_speaker(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-]\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def build_reference_embeddings_from_diarization(\n",
    "    original_audio_path: str,\n",
    "    gemini_data: dict,\n",
    "    out_dir: str = \"reference_segments\",\n",
    "    join_silence_ms: int = 100,\n",
    "    sample_rate: int = 24000,\n",
    "    min_total_duration_sec: float = 2.5,\n",
    "    tone_color_converter=tone_color_converter,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate all segments for each speaker (in chronological order),\n",
    "    export a single WAV reference per speaker, then create + save OpenVoice embeddings.\n",
    "\n",
    "    Returns:\n",
    "      dict mapping speaker -> {\"ref_wav\": str, \"embedding\": str, \"duration_s\": float}\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load full original audio once (pydub uses ffmpeg behind the scenes)\n",
    "    audio = AudioSegment.from_file(str(original_audio_path))\n",
    "    full_duration_ms = len(audio)\n",
    "    print(f\"[info] Loaded original audio '{original_audio_path}' ({full_duration_ms/1000:.2f}s)\")\n",
    "\n",
    "    # Group segments by speaker (keeps order)\n",
    "    speaker_segments = defaultdict(list)\n",
    "    for seg in gemini_data.get(\"speak_segments\", []):\n",
    "        try:\n",
    "            start_ms = int(round(float(seg[\"start_time\"]) * 1000))\n",
    "            end_ms   = int(round(float(seg[\"end_time\"]) * 1000))\n",
    "        except Exception as e:\n",
    "            print(f\"[warning] skipping segment with bad times: {seg} -> {e}\")\n",
    "            continue\n",
    "        if end_ms <= start_ms:\n",
    "            print(f\"[warning] skipping zero/negative-length segment: start={start_ms} end={end_ms}\")\n",
    "            continue\n",
    "        # clamp within audio\n",
    "        start_ms = max(0, min(start_ms, full_duration_ms))\n",
    "        end_ms   = max(0, min(end_ms, full_duration_ms))\n",
    "        dur_ms = end_ms - start_ms\n",
    "        speaker = seg.get(\"speaker\", \"unknown\").replace(' ', '_')\n",
    "        speaker_segments[speaker].append({\"start_ms\": start_ms, \"end_ms\": end_ms, \"duration_ms\": dur_ms})\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for speaker, segs in speaker_segments.items():\n",
    "        # sort by start time to keep natural order\n",
    "        segs_sorted = sorted(segs, key=lambda x: x[\"start_ms\"])\n",
    "        concat = AudioSegment.silent(duration=0, frame_rate=sample_rate)\n",
    "\n",
    "        total_ms = 0\n",
    "        for s in segs_sorted:\n",
    "            start_ms = s[\"start_ms\"]\n",
    "            end_ms = s[\"end_ms\"]\n",
    "            # slice original audio\n",
    "            clip = audio[start_ms:end_ms]\n",
    "            # append clip\n",
    "            concat += clip\n",
    "            total_ms += len(clip)\n",
    "            # add tiny silence between clips to avoid gluing words (optional)\n",
    "            concat += AudioSegment.silent(duration=join_silence_ms, frame_rate=sample_rate)\n",
    "\n",
    "        total_s = total_ms / 1000.0\n",
    "\n",
    "        # Warn if too short\n",
    "        if total_s < min_total_duration_sec:\n",
    "            warnings.warn(\n",
    "                f\"[warn] total concatenated duration for speaker '{speaker}' is short ({total_s:.2f}s). \"\n",
    "                \"Embeddings may be poor. Consider merging more segments or increasing min_total_duration_sec.\"\n",
    "            )\n",
    "\n",
    "        # Trim trailing silence\n",
    "        # If concat is longer than join silence at end, remove last join_silence_ms\n",
    "        if len(concat) >= join_silence_ms:\n",
    "            concat = concat[:-join_silence_ms]\n",
    "\n",
    "        # Normalize export settings\n",
    "        concat = concat.set_frame_rate(sample_rate).set_channels(1)\n",
    "\n",
    "        # Save reference wav\n",
    "        speaker_safe = _sanitize_speaker(speaker)\n",
    "        ref_path = out_dir / f\"ref_{speaker_safe}.wav\"\n",
    "        concat.export(str(ref_path), format=\"wav\")\n",
    "        print(f\"[ok] Saved ref audio for '{speaker}' -> {ref_path} ({total_s:.2f}s)\")\n",
    "\n",
    "        # Create embedding via se_extractor\n",
    "        try:\n",
    "            # se_extractor.get_se may accept different signatures; attempt with tone_color_converter first\n",
    "            if tone_color_converter is not None:\n",
    "                source_se, audio_name = se_extractor.get_se(str(ref_path), tone_color_converter, vad=True)\n",
    "            else:\n",
    "                # fallback signature\n",
    "                source_se, audio_name = se_extractor.get_se(str(ref_path), vad=True)\n",
    "        except TypeError:\n",
    "            # try alternate call (some builds expect only path)\n",
    "            source_se, audio_name = se_extractor.get_se(str(ref_path))\n",
    "\n",
    "        emb_path = out_dir / f\"{speaker_safe}_embedding.pt\"\n",
    "        torch.save(source_se, str(emb_path))\n",
    "        print(f\"[ok] Saved embedding for '{speaker}' -> {emb_path}\")\n",
    "\n",
    "        results[speaker] = {\n",
    "            \"ref_wav\": str(ref_path),\n",
    "            \"embedding\": str(emb_path),\n",
    "            \"duration_s\": total_s,\n",
    "            \"num_segments\": len(segs_sorted),\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loaded original audio 'media_1_audio.mp3' (35.11s)\n",
      "[ok] Saved ref audio for 'Speaker_1' -> reference_segments/ref_Speaker_1.wav (7.52s)\n",
      "OpenVoice version: v2\n",
      "[(0.0, 7.72)]\n",
      "after vad: dur = 7.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skamalj/miniconda3/envs/openvoice/lib/python3.9/site-packages/torch/functional.py:730: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /pytorch/aten/src/ATen/native/SpectralOps.cpp:875.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved embedding for 'Speaker_1' -> reference_segments/Speaker_1_embedding.pt\n",
      "[ok] Saved ref audio for 'Speaker_2' -> reference_segments/ref_Speaker_2.wav (12.78s)\n",
      "OpenVoice version: v2\n",
      "[(0.0, 12.98)]\n",
      "after vad: dur = 12.98\n",
      "[ok] Saved embedding for 'Speaker_2' -> reference_segments/Speaker_2_embedding.pt\n",
      "[ok] Saved ref audio for 'Speaker_3' -> reference_segments/ref_Speaker_3.wav (5.62s)\n",
      "OpenVoice version: v2\n",
      "[(0.0, 5.62)]\n",
      "after vad: dur = 5.62\n",
      "[ok] Saved embedding for 'Speaker_3' -> reference_segments/Speaker_3_embedding.pt\n",
      "{'Speaker_1': {'ref_wav': 'reference_segments/ref_Speaker_1.wav', 'embedding': 'reference_segments/Speaker_1_embedding.pt', 'duration_s': 7.52, 'num_segments': 3}, 'Speaker_2': {'ref_wav': 'reference_segments/ref_Speaker_2.wav', 'embedding': 'reference_segments/Speaker_2_embedding.pt', 'duration_s': 12.78, 'num_segments': 3}, 'Speaker_3': {'ref_wav': 'reference_segments/ref_Speaker_3.wav', 'embedding': 'reference_segments/Speaker_3_embedding.pt', 'duration_s': 5.62, 'num_segments': 1}}\n"
     ]
    }
   ],
   "source": [
    "# gemini_data = json.load(open(\"translated_diarized_data.json\", encoding=\"utf-8\"))\n",
    "# original_audio = \"original_audio.mp3\"\n",
    "\n",
    "ref_map = build_reference_embeddings_from_diarization(\n",
    "    original_audio_path=\"media_1_audio.mp3\",\n",
    "    gemini_data=gemini_data,\n",
    "    out_dir=\"reference_segments\",\n",
    "    join_silence_ms=100,\n",
    "    sample_rate=24000,\n",
    "    min_total_duration_sec=5.0,\n",
    "    tone_color_converter=tone_color_converter  # or None\n",
    ")\n",
    "\n",
    "print(ref_map)\n",
    "# -> {'Speaker 1': {'ref_wav': 'reference_segments/ref_Speaker_1.wav', 'embedding': 'reference_segments/Speaker_1_embedding.pt', ...}, ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def build_src_embedding_from_tts(tts_dir: str, gemini_data: dict, tone_color_converter=None, max_len_sec=10):\n",
    "    \"\"\"\n",
    "    Combine TTS segments (up to max_len_sec) to create one source embedding.\n",
    "    \"\"\"\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "    total_ms = 0\n",
    "\n",
    "    \n",
    "    for idx, seg in enumerate(gemini_data[\"speak_segments\"]):\n",
    "        speaker = seg.get(\"speaker\", \"Speaker\").replace(' ', '_')\n",
    "        tts_path = Path(tts_dir) / f\"{idx}_{speaker}.wav\"\n",
    "        if not tts_path.exists():\n",
    "            continue\n",
    "\n",
    "        seg_audio = AudioSegment.from_wav(tts_path)\n",
    "        combined += seg_audio\n",
    "        total_ms += len(seg_audio)\n",
    "\n",
    "        if total_ms >= max_len_sec * 1000:\n",
    "            break\n",
    "\n",
    "    # Save temporary combined TTS clip\n",
    "    tmp_path = Path(tts_dir) / \"tts_src_reference.wav\"\n",
    "    combined.export(tmp_path, format=\"wav\")\n",
    "\n",
    "    # Extract embedding once\n",
    "    if tone_color_converter is not None:\n",
    "        src_se, _ = se_extractor.get_se(str(tmp_path), tone_color_converter, vad=True)\n",
    "    else:\n",
    "        src_se, _ = se_extractor.get_se(str(tmp_path), vad=True)\n",
    "\n",
    "    print(f\"[ok] Source embedding created from {total_ms/1000:.1f}s of TTS audio -> {tmp_path}\")\n",
    "    return src_se, str(tmp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v2\n",
      "[(0.0, 10.7019375)]\n",
      "after vad: dur = 10.701\n",
      "[ok] Source embedding created from 10.7s of TTS audio -> translated_segments/tts_src_reference.wav\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build **single src embedding** from TTS segments\n",
    "src_se, tts_ref_path = build_src_embedding_from_tts(\n",
    "    tts_dir=\"translated_segments\",\n",
    "    gemini_data=gemini_data,\n",
    "    tone_color_converter=tone_color_converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tts_segments_with_refs(tts_dir, gemini_data, ref_map, out_dir, src_se, tone_color_converter):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "    for idx, seg in enumerate(gemini_data.get(\"speak_segments\", [])):\n",
    "        speaker = seg[\"speaker\"].replace(' ', '_')\n",
    "\n",
    "        tts_wav = Path(tts_dir) / f\"adjusted_seg_{idx}_{speaker}_stretched.wav\"\n",
    "        if not tts_wav.exists():\n",
    "            print(f\"[skip] No TTS audio for seg {idx}\")\n",
    "            continue\n",
    "\n",
    "        if speaker not in ref_map:\n",
    "            print(f\"[skip] No reference for {speaker}\")\n",
    "            continue\n",
    "\n",
    "        tgt_se = torch.load(ref_map[speaker][\"embedding\"])\n",
    "        out_wav = out_dir / f\"converted_seg_{idx}_{speaker}.wav\"\n",
    "\n",
    "        tone_color_converter.convert(\n",
    "            audio_src_path=str(tts_wav),\n",
    "            src_se=src_se,   # <-- reuse the same source embedding\n",
    "            tgt_se=tgt_se,\n",
    "            output_path=str(out_wav),\n",
    "            message=\"@MyShell\",\n",
    "        )\n",
    "        print(f\"[ok] Converted seg {idx} ({speaker}) -> {out_wav}\")\n",
    "        results[idx] = str(out_wav)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Converted seg 0 (Speaker_1) -> converted_segments/converted_seg_0_Speaker_1.wav\n",
      "[ok] Converted seg 1 (Speaker_2) -> converted_segments/converted_seg_1_Speaker_2.wav\n",
      "Audio too short, fail to add watermark\n",
      "[ok] Converted seg 2 (Speaker_1) -> converted_segments/converted_seg_2_Speaker_1.wav\n",
      "[ok] Converted seg 3 (Speaker_2) -> converted_segments/converted_seg_3_Speaker_2.wav\n",
      "[ok] Converted seg 4 (Speaker_1) -> converted_segments/converted_seg_4_Speaker_1.wav\n",
      "[ok] Converted seg 5 (Speaker_2) -> converted_segments/converted_seg_5_Speaker_2.wav\n",
      "[ok] Converted seg 6 (Speaker_3) -> converted_segments/converted_seg_6_Speaker_3.wav\n"
     ]
    }
   ],
   "source": [
    "converted = convert_tts_segments_with_refs(\n",
    "    tts_dir=\"adjusted_segments\",\n",
    "    gemini_data=gemini_data,\n",
    "    ref_map=ref_map,\n",
    "    out_dir=\"converted_segments\",\n",
    "    src_se=src_se,\n",
    "    tone_color_converter=tone_color_converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlayed segment 0 (Speaker 1) at 320 ms\n",
      "Overlayed segment 1 (Speaker 2) at 4990 ms\n",
      "Overlayed segment 2 (Speaker 1) at 8560 ms\n",
      "Overlayed segment 3 (Speaker 2) at 10220 ms\n",
      "Overlayed segment 4 (Speaker 1) at 16320 ms\n",
      "Overlayed segment 5 (Speaker 2) at 19460 ms\n",
      "Overlayed segment 6 (Speaker 3) at 26336 ms\n",
      "Final audio mix saved to final_mix.wav\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def mix_segments_with_background(gemini_json_path, converted_dir, background_audio_path, output_path):\n",
    "    # Load segment metadata\n",
    "    with open(gemini_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gemini_data = json.load(f)\n",
    "\n",
    "    speak_segments = gemini_data[\"speak_segments\"]\n",
    "\n",
    "    # Load background audio\n",
    "    bg_audio = AudioSegment.from_file(background_audio_path, format=\"mp3\")\n",
    "    \n",
    "    # Overlay each converted segment\n",
    "    for idx, seg in enumerate(speak_segments):\n",
    "        speaker = seg[\"speaker\"].replace(' ', '_')\n",
    "        seg_file = Path(converted_dir) / f\"converted_seg_{idx}_{speaker}.wav\"\n",
    "        if not seg_file.exists():\n",
    "            print(f\"Skipping missing segment: {seg_file}\")\n",
    "            continue\n",
    "        \n",
    "        segment_audio = AudioSegment.from_file(seg_file, format=\"wav\")\n",
    "        start_ms = int(float(seg[\"start_time\"]) * 1000)\n",
    "        bg_audio = bg_audio.overlay(segment_audio, position=start_ms)\n",
    "        print(f\"Overlayed segment {idx} ({seg['speaker']}) at {start_ms} ms\")\n",
    "\n",
    "    # Export final mix\n",
    "    bg_audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Final audio mix saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "mix_segments_with_background(\n",
    "    gemini_json_path=\"translated_diarized_data.json\",\n",
    "    converted_dir=\"converted_segments\",\n",
    "    background_audio_path=\"separated_audio/htdemucs/media_1_audio/no_vocals.wav\",\n",
    "    output_path=\"final_mix.wav\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video output_video_with_audio.mp4.\n",
      "MoviePy - Writing audio in output_video_with_audioTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video output_video_with_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready output_video_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Load video and audio clips\n",
    "video = VideoFileClip(\"media_1_video.mp4\")\n",
    "audio = AudioFileClip(\"final_mix.wav\")\n",
    "\n",
    "# Set the new audio to the video\n",
    "final_video = video.with_audio(audio)\n",
    "\n",
    "# Export the combined file\n",
    "final_video.write_videofile(\"output_video_with_audio.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tataplay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
