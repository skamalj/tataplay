{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client =  genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "myfile = client.files.upload(file='/home/skamalj/dev/tataplay/separated_audio/htdemucs/media_1_audio/vocals.wav')\n",
    "\n",
    "response = client.models.generate_content(\n",
    "  model='gemini-2.5-pro',\n",
    "  contents=[\n",
    "    \"\"\"You are given an audio clip. \n",
    "    Task:\n",
    "    1. Identify the speakers.\n",
    "    2. Give them indian names basis gender\n",
    "    3. Descrive their speaking style\n",
    "\n",
    "    Stye description examples:\n",
    "    Aditi - Slightly High-Pitched, Expressive Tone:\n",
    "    \"Aditi speaks with a slightly higher pitch in a close-sounding environment. Her voice is clear, with subtle emotional depth and a normal pace, all captured in high-quality recording.\"\n",
    "\n",
    "    Sita - Rapid, Slightly Monotone:\n",
    "    \"Sita speaks at a fast pace with a slightly low-pitched voice, captured clearly in a close-sounding environment with excellent recording quality.\"\n",
    "\n",
    "    Tapan - Male, Moderate Pace, Slightly Monotone:\n",
    "    \"Tapan speaks at a moderate pace with a slightly monotone tone. The recording is clear, with a close sound and only minimal ambient noise.\"\n",
    "    \"\"\",\n",
    "    myfile,\n",
    "  ]\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client =  genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "myfile = client.files.upload(file='/home/skamalj/dev/tataplay/separated_audio/htdemucs/media_1_audio/vocals.wav')\n",
    "\n",
    "language = \"English\"\n",
    "response = client.models.generate_content(\n",
    "  model='gemini-2.5-pro',\n",
    "  contents=[\n",
    "f\"\"\"\n",
    "You are given an audio clip. Perform the following tasks:\n",
    "\n",
    "1. Transcribe the speech in its original language.\n",
    "2. Translate into the target language: {language}, following these rules:\n",
    "   - Preserve any code-switched words exactly (e.g., English words spoken inside Hindi).\n",
    "   - Keep the translation natural and conversational.\n",
    "   - Aim to match the original speech length (word count and duration) as closely as possible,\n",
    "     without making the translation sound forced.\n",
    "   - Ensure the translated_text remains in {language}, even if code-switched words are included.\n",
    "3. Assume 3 speakers (Speaker 1, Speaker 2, Speaker 3) and perform diarization.\n",
    "4. Generate SSML for each segment:\n",
    "   - Use <speak> and a unique <voice> per speaker.\n",
    "   - Insert <break> and <prosody> to reflect pauses, pitch, and tone.\n",
    "   - Wrap the full text in a <lang xml:lang=\"...\"> tag for {language}.\n",
    "   - If a preserved word may cause mispronunciation or language switching,\n",
    "     wrap it with <phoneme> or <lang> as needed.\n",
    "\n",
    "Return strictly valid JSON in this format:\n",
    "\n",
    "{{\n",
    "  \"speakers_count\": <int>,\n",
    "  \"speak_segments\": [\n",
    "    {{\n",
    "      \"speaker\": \"<speaker_id>\",\n",
    "      \"start_time\": <float_seconds>,\n",
    "      \"end_time\": <float_seconds>,\n",
    "      \"original_text\": \"<transcribed_text>\",\n",
    "      \"translated_text\": \"<translation_in_{language}_with_code-switched_words_preserved>\",\n",
    "      \"ssml\": \"<speak><voice name='...'><lang xml:lang='xx-XX'>...</lang></voice></speak>\"\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"metadata\": {{\n",
    "    \"translation_language\": \"{language}\",\n",
    "    \"total_duration\": <float_seconds>,\n",
    "    \"notes\": \"code-switched words preserved; pitch, pauses, and tone captured; translation length aligned to original\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Every segment must include original_text, translated_text, and ssml.\n",
    "- Preserve code-switched words exactly.\n",
    "- Ensure translated_text is in {language} (not influenced by preserved words).\n",
    "- SSML must enforce the target language (<lang xml:lang=\"...\">).\n",
    "- Keep translations concise and aligned with original timing.\n",
    "- No explanation outside the JSON.\n",
    "\"\"\",\n",
    "    myfile\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions (just run that cell)\n",
    "\n",
    "import contextlib\n",
    "import wave\n",
    "from IPython.display import Audio\n",
    "\n",
    "file_index = 0\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def wave_file(filename, channels=1, rate=24000, sample_width=2):\n",
    "    with wave.open(filename, \"wb\") as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(sample_width)\n",
    "        wf.setframerate(rate)\n",
    "        yield wf\n",
    "\n",
    "def play_audio_blob(blob):\n",
    "  global file_index\n",
    "  file_index += 1\n",
    "\n",
    "  fname = f'audio_{file_index}.wav'\n",
    "  with wave_file(fname) as wav:\n",
    "    wav.writeframes(blob.data)\n",
    "\n",
    "  return Audio(fname, autoplay=True)\n",
    "\n",
    "def play_audio(response):\n",
    "    return play_audio_blob(response.candidates[0].content.parts[0].inline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example: Gemini response text\n",
    "text = response.text\n",
    "\n",
    "# Remove leading ```json and trailing ```\n",
    "if text.startswith(\"```json\"):\n",
    "    text = text[len(\"```json\"):].strip()\n",
    "if text.endswith(\"```\"):\n",
    "    text = text[:-3].strip()\n",
    "\n",
    "# Now parse as JSON\n",
    "gemini_data = json.loads(text)\n",
    "\n",
    "# save gemini_data to a file\n",
    "with open('translated_diarized_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(gemini_data, f, indent=2,ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n",
    "   with wave.open(filename, \"wb\") as wf:\n",
    "      wf.setnchannels(channels)\n",
    "      wf.setsampwidth(sample_width)\n",
    "      wf.setframerate(rate)\n",
    "      wf.writeframes(pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "from pathlib import Path\n",
    "\n",
    "def synthesize_segment_tts(client, segment, output_dir=\"translated_segments\", voice_name=\"Puck\"):\n",
    "    \"\"\"\n",
    "    Generate TTS audio for a single segment, keeping duration for lip-sync.\n",
    "\n",
    "    segment: dict with keys\n",
    "        - ssml: SSML string\n",
    "        - start_time: float (seconds)\n",
    "        - end_time: float (seconds)\n",
    "        - speaker: string\n",
    "    client: gemini client\n",
    "    output_dir: where to save the segment audio\n",
    "    voice_name: prebuilt Gemini TTS voice\n",
    "    \"\"\"\n",
    "    import os\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Calculate duration from segment metadata\n",
    "    duration = float(segment[\"end_time\"])  - float(segment[\"start_time\"])\n",
    "    \n",
    "    # Prepare output file path\n",
    "    seg_index = segment.get(\"index\", 0)\n",
    "\n",
    "    speaker = segment.get(\"speaker\", \"Speaker\")\n",
    "    out_file = Path(output_dir) / f\"{seg_index}_{speaker.replace(' ', '_')}.wav\"\n",
    "\n",
    "    print(f\"Processing segment {seg_index} with duration {duration:.3f} seconds to {out_file}\")\n",
    "    print(segment['translated_text'])\n",
    "    # Wrap the SSML into the TTS prompt\n",
    "    tts_prompt = f\"\"\"\n",
    "    You are given SSML with timing metadata. \n",
    "    Identify the target language from text. If language is english ensure that accent is **Indian English**.\n",
    "    Must keep original pauses and restrict **durations to {duration:.3f} seconds**. \n",
    "    Generate **audio output**.\n",
    "\n",
    "    Here is the SSML:\n",
    "\n",
    "    {segment['translated_text']}\n",
    "    \"\"\"\n",
    "    #gemini-2.5-flash-preview-tts\n",
    "    #emini-2.5-pro-preview-tts\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-pro-preview-tts',\n",
    "        contents=tts_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=[\"AUDIO\"],\n",
    "            speech_config=types.SpeechConfig(\n",
    "                voice_config=types.VoiceConfig(\n",
    "                    prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
    "                        voice_name=voice_name,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    print(response.candidates[0])\n",
    "    # Save the returned audio bytes\n",
    "    if response.candidates[0].content:\n",
    "        data = response.candidates[0].content.parts[0].inline_data.data # Gemini returns audio bytes in response.audio\n",
    "        create_wave_file(str(out_file), data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm translated_segments/*.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block is only for testing. Run for any single segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3  # change this index as needed\n",
    "seg = gemini_data[\"speak_segments\"][n]\n",
    "seg[\"index\"] = n\n",
    "\n",
    "synthesize_segment_tts(client, seg, voice_name=\"Puck\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os\n",
    "client =  genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `gemini_data` is your parsed Gemini JSON with speak_segments\n",
    "for idx, seg in enumerate(gemini_data[\"speak_segments\"]):\n",
    "    seg[\"index\"] = idx\n",
    "    synthesize_segment_tts(client, seg, voice_name=\"Puck\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to stretch a audio for segments. WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "from audiostretchy.stretch import stretch_audio\n",
    "\n",
    "def remove_silence_from_end(audio_path, output_path, silence_thresh=-50, chunk_size=10):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    silence_ranges = detect_silence(audio, min_silence_len=chunk_size, silence_thresh=silence_thresh)\n",
    "\n",
    "    # Default end_trim is full length of audio\n",
    "    end_trim = len(audio)\n",
    "\n",
    "    # Check if last silence segment is at the end of audio\n",
    "    if silence_ranges and silence_ranges[-1][1] == len(audio):\n",
    "        # Trim silence at the end\n",
    "        end_trim = silence_ranges[-1][0]\n",
    "\n",
    "    # Keep audio from start to end_trim (remove silence only at end)\n",
    "    trimmed_audio = audio[:end_trim]\n",
    "    trimmed_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "# Usage example:\n",
    "#remove_silence_from_end(\"translated_segments/1_Speaker_2_compress.wav\", \"translated_segments/trimmed_end_only.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved adjusted segment: adjusted_segments/adjusted_seg_0_Speaker_1.wav (target: 1.13s, actual: 1.25s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_1_Speaker_1.wav (target: 2.05s, actual: 3.29s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_2_Speaker_1.wav (target: 5.02s, actual: 6.45s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_3_Speaker_2.wav (target: 4.52s, actual: 4.85s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_4_Speaker_2.wav (target: 0.79s, actual: 2.45s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_5_Speaker_1.wav (target: 2.43s, actual: 3.65s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_6_Speaker_2.wav (target: 4.13s, actual: 4.09s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_7_Speaker_3.wav (target: 1.87s, actual: 3.21s)\n",
      "Saved adjusted segment: adjusted_segments/adjusted_seg_8_Speaker_3.wav (target: 3.24s, actual: 4.81s)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "from audiostretchy.stretch import stretch_audio\n",
    "\n",
    "# Paths\n",
    "json_path = \"translated_diarized_data.json\"\n",
    "tts_dir = Path(\"translated_segments\")          # TTS-generated segments\n",
    "adjusted_dir = Path(\"adjusted_segments\")      # Save adjusted segments\n",
    "adjusted_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load JSON\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "segments = data[\"speak_segments\"]\n",
    "\n",
    "\n",
    "for idx, seg in enumerate(segments):\n",
    "    speaker = seg.get(\"speaker\", \"Speaker\")\n",
    "    \n",
    "    tts_file = tts_dir / f\"{idx}_{speaker.replace(' ', '_')}.wav\"\n",
    "    if not tts_file.exists():\n",
    "        print(f\"Missing file: {tts_file}\")\n",
    "        continue\n",
    "\n",
    "    # Target duration from JSON\n",
    "    target_duration = float(seg[\"end_time\"]) - float(seg[\"start_time\"])  # in seconds\n",
    "\n",
    "   # Load your audio file\n",
    "    audio = AudioSegment.from_file(tts_file)\n",
    "\n",
    "    # Get the duration in milliseconds\n",
    "    current_duration = audio.duration_seconds\n",
    "\n",
    "    # Save adjusted segment\n",
    "    out_file = adjusted_dir / f\"adjusted_seg_{idx}_{seg['speaker'].replace(' ', '_')}.wav\"\n",
    "    out_file_stretched = adjusted_dir / f\"adjusted_seg_{idx}_{seg['speaker'].replace(' ', '_')}_stretched.wav\"\n",
    "\n",
    "    stretch_factor = max(0.7, min(1.3, target_duration / current_duration))\n",
    "    adjusted_audio = stretch_audio(tts_file, out_file, ratio=stretch_factor)\n",
    "    remove_silence_from_end(out_file, out_file_stretched)\n",
    "\n",
    "    print(f\"Saved adjusted segment: {out_file} (target: {target_duration:.2f}s, actual: {current_duration:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to OpenVoice Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 segments from file\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------- Load Gemini segments -----------\n",
    "with open(\"translated_diarized_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gemini_data = json.load(f)\n",
    "\n",
    "segments = gemini_data.get(\"speak_segments\", [])\n",
    "\n",
    "print(f\"Loaded {len(segments)} segments from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skamalj/.conda/envs/openvoice/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints_v2/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from openvoice.api import ToneColorConverter\n",
    "\n",
    "ckpt_converter = 'checkpoints_v2/converter'\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# OpenVoice se_extractor import (make sure OpenVoice is on PYTHONPATH)\n",
    "from openvoice import se_extractor\n",
    "\n",
    "def _sanitize_speaker(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-]\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def build_reference_embeddings_from_diarization(\n",
    "    original_audio_path: str,\n",
    "    gemini_data: dict,\n",
    "    out_dir: str = \"reference_segments\",\n",
    "    join_silence_ms: int = 100,\n",
    "    sample_rate: int = 24000,\n",
    "    min_total_duration_sec: float = 2.5,\n",
    "    tone_color_converter=tone_color_converter,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate all segments for each speaker (in chronological order),\n",
    "    export a single WAV reference per speaker, then create + save OpenVoice embeddings.\n",
    "\n",
    "    Returns:\n",
    "      dict mapping speaker -> {\"ref_wav\": str, \"embedding\": str, \"duration_s\": float}\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load full original audio once (pydub uses ffmpeg behind the scenes)\n",
    "    audio = AudioSegment.from_file(str(original_audio_path))\n",
    "    full_duration_ms = len(audio)\n",
    "    print(f\"[info] Loaded original audio '{original_audio_path}' ({full_duration_ms/1000:.2f}s)\")\n",
    "\n",
    "    # Group segments by speaker (keeps order)\n",
    "    speaker_segments = defaultdict(list)\n",
    "    for seg in gemini_data.get(\"speak_segments\", []):\n",
    "        try:\n",
    "            start_ms = int(round(float(seg[\"start_time\"]) * 1000))\n",
    "            end_ms   = int(round(float(seg[\"end_time\"]) * 1000))\n",
    "        except Exception as e:\n",
    "            print(f\"[warning] skipping segment with bad times: {seg} -> {e}\")\n",
    "            continue\n",
    "        if end_ms <= start_ms:\n",
    "            print(f\"[warning] skipping zero/negative-length segment: start={start_ms} end={end_ms}\")\n",
    "            continue\n",
    "        # clamp within audio\n",
    "        start_ms = max(0, min(start_ms, full_duration_ms))\n",
    "        end_ms   = max(0, min(end_ms, full_duration_ms))\n",
    "        dur_ms = end_ms - start_ms\n",
    "        speaker = seg.get(\"speaker\", \"unknown\").replace(' ', '_')\n",
    "        speaker_segments[speaker].append({\"start_ms\": start_ms, \"end_ms\": end_ms, \"duration_ms\": dur_ms})\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for speaker, segs in speaker_segments.items():\n",
    "        # sort by start time to keep natural order\n",
    "        segs_sorted = sorted(segs, key=lambda x: x[\"start_ms\"])\n",
    "        concat = AudioSegment.silent(duration=0, frame_rate=sample_rate)\n",
    "\n",
    "        total_ms = 0\n",
    "        for s in segs_sorted:\n",
    "            start_ms = s[\"start_ms\"]\n",
    "            end_ms = s[\"end_ms\"]\n",
    "            # slice original audio\n",
    "            clip = audio[start_ms:end_ms]\n",
    "            # append clip\n",
    "            concat += clip\n",
    "            total_ms += len(clip)\n",
    "            # add tiny silence between clips to avoid gluing words (optional)\n",
    "            concat += AudioSegment.silent(duration=join_silence_ms, frame_rate=sample_rate)\n",
    "\n",
    "        total_s = total_ms / 1000.0\n",
    "\n",
    "        # Warn if too short\n",
    "        if total_s < min_total_duration_sec:\n",
    "            warnings.warn(\n",
    "                f\"[warn] total concatenated duration for speaker '{speaker}' is short ({total_s:.2f}s). \"\n",
    "                \"Embeddings may be poor. Consider merging more segments or increasing min_total_duration_sec.\"\n",
    "            )\n",
    "\n",
    "        # Trim trailing silence\n",
    "        # If concat is longer than join silence at end, remove last join_silence_ms\n",
    "        if len(concat) >= join_silence_ms:\n",
    "            concat = concat[:-join_silence_ms]\n",
    "\n",
    "        # Normalize export settings\n",
    "        concat = concat.set_frame_rate(sample_rate).set_channels(1)\n",
    "\n",
    "        # Save reference wav\n",
    "        speaker_safe = _sanitize_speaker(speaker)\n",
    "        ref_path = out_dir / f\"ref_{speaker_safe}.wav\"\n",
    "        concat.export(str(ref_path), format=\"wav\")\n",
    "        print(f\"[ok] Saved ref audio for '{speaker}' -> {ref_path} ({total_s:.2f}s)\")\n",
    "\n",
    "        # Create embedding via se_extractor\n",
    "        try:\n",
    "            # se_extractor.get_se may accept different signatures; attempt with tone_color_converter first\n",
    "            if tone_color_converter is not None:\n",
    "                source_se, audio_name = se_extractor.get_se(str(ref_path), tone_color_converter, vad=True)\n",
    "            else:\n",
    "                # fallback signature\n",
    "                source_se, audio_name = se_extractor.get_se(str(ref_path), vad=True)\n",
    "        except TypeError:\n",
    "            # try alternate call (some builds expect only path)\n",
    "            source_se, audio_name = se_extractor.get_se(str(ref_path))\n",
    "\n",
    "        emb_path = out_dir / f\"{speaker_safe}_embedding.pt\"\n",
    "        torch.save(source_se, str(emb_path))\n",
    "        print(f\"[ok] Saved embedding for '{speaker}' -> {emb_path}\")\n",
    "\n",
    "        results[speaker] = {\n",
    "            \"ref_wav\": str(ref_path),\n",
    "            \"embedding\": str(emb_path),\n",
    "            \"duration_s\": total_s,\n",
    "            \"num_segments\": len(segs_sorted),\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loaded original audio 'media_1_audio.mp3' (35.11s)\n",
      "[ok] Saved ref audio for 'Speaker_1' -> reference_segments/ref_Speaker_1.wav (10.63s)\n",
      "OpenVoice version: v2\n",
      "[(0.11, 6.642), (6.83, 10.93)]\n",
      "after vad: dur = 10.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skamalj/.conda/envs/openvoice/lib/python3.9/site-packages/torch/functional.py:730: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /pytorch/aten/src/ATen/native/SpectralOps.cpp:875.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved embedding for 'Speaker_1' -> reference_segments/Speaker_1_embedding.pt\n",
      "[ok] Saved ref audio for 'Speaker_2' -> reference_segments/ref_Speaker_2.wav (9.44s)\n",
      "OpenVoice version: v2\n",
      "[(0.0, 9.64)]\n",
      "after vad: dur = 9.64\n",
      "[ok] Saved embedding for 'Speaker_2' -> reference_segments/Speaker_2_embedding.pt\n",
      "[ok] Saved ref audio for 'Speaker_3' -> reference_segments/ref_Speaker_3.wav (5.11s)\n",
      "OpenVoice version: v2\n",
      "[(0.0, 5.21)]\n",
      "after vad: dur = 5.21\n",
      "[ok] Saved embedding for 'Speaker_3' -> reference_segments/Speaker_3_embedding.pt\n",
      "{'Speaker_1': {'ref_wav': 'reference_segments/ref_Speaker_1.wav', 'embedding': 'reference_segments/Speaker_1_embedding.pt', 'duration_s': 10.63, 'num_segments': 4}, 'Speaker_2': {'ref_wav': 'reference_segments/ref_Speaker_2.wav', 'embedding': 'reference_segments/Speaker_2_embedding.pt', 'duration_s': 9.44, 'num_segments': 3}, 'Speaker_3': {'ref_wav': 'reference_segments/ref_Speaker_3.wav', 'embedding': 'reference_segments/Speaker_3_embedding.pt', 'duration_s': 5.11, 'num_segments': 2}}\n"
     ]
    }
   ],
   "source": [
    "# gemini_data = json.load(open(\"translated_diarized_data.json\", encoding=\"utf-8\"))\n",
    "# original_audio = \"original_audio.mp3\"\n",
    "\n",
    "ref_map = build_reference_embeddings_from_diarization(\n",
    "    original_audio_path=\"media_1_audio.mp3\",\n",
    "    gemini_data=gemini_data,\n",
    "    out_dir=\"reference_segments\",\n",
    "    join_silence_ms=100,\n",
    "    sample_rate=24000,\n",
    "    min_total_duration_sec=5.0,\n",
    "    tone_color_converter=tone_color_converter  # or None\n",
    ")\n",
    "\n",
    "print(ref_map)\n",
    "# -> {'Speaker 1': {'ref_wav': 'reference_segments/ref_Speaker_1.wav', 'embedding': 'reference_segments/Speaker_1_embedding.pt', ...}, ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def build_src_embedding_from_tts(tts_dir: str, gemini_data: dict, tone_color_converter=None, max_len_sec=10):\n",
    "    \"\"\"\n",
    "    Combine TTS segments (up to max_len_sec) to create one source embedding.\n",
    "    \"\"\"\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "    total_ms = 0\n",
    "\n",
    "    \n",
    "    for idx, seg in enumerate(gemini_data[\"speak_segments\"]):\n",
    "        speaker = seg.get(\"speaker\", \"Speaker\").replace(' ', '_')\n",
    "        tts_path = Path(tts_dir) / f\"{idx}_{speaker}.wav\"\n",
    "        if not tts_path.exists():\n",
    "            continue\n",
    "\n",
    "        seg_audio = AudioSegment.from_wav(tts_path)\n",
    "        combined += seg_audio\n",
    "        total_ms += len(seg_audio)\n",
    "\n",
    "        if total_ms >= max_len_sec * 1000:\n",
    "            break\n",
    "\n",
    "    # Save temporary combined TTS clip\n",
    "    tmp_path = Path(tts_dir) / \"tts_src_reference.wav\"\n",
    "    combined.export(tmp_path, format=\"wav\")\n",
    "\n",
    "    # Extract embedding once\n",
    "    if tone_color_converter is not None:\n",
    "        src_se, _ = se_extractor.get_se(str(tmp_path), tone_color_converter, vad=True)\n",
    "    else:\n",
    "        src_se, _ = se_extractor.get_se(str(tmp_path), vad=True)\n",
    "\n",
    "    print(f\"[ok] Source embedding created from {total_ms/1000:.1f}s of TTS audio -> {tmp_path}\")\n",
    "    return src_se, str(tmp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v2\n",
      "[(0.0, 10.992875)]\n",
      "after vad: dur = 10.992\n",
      "[ok] Source embedding created from 11.0s of TTS audio -> translated_segments/tts_src_reference.wav\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build **single src embedding** from TTS segments\n",
    "src_se, tts_ref_path = build_src_embedding_from_tts(\n",
    "    tts_dir=\"translated_segments\",\n",
    "    gemini_data=gemini_data,\n",
    "    tone_color_converter=tone_color_converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tts_segments_with_refs(tts_dir, gemini_data, ref_map, out_dir, src_se, tone_color_converter):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "    for idx, seg in enumerate(gemini_data.get(\"speak_segments\", [])):\n",
    "        speaker = seg[\"speaker\"].replace(' ', '_')\n",
    "\n",
    "        tts_wav = Path(tts_dir) / f\"adjusted_seg_{idx}_{speaker}_stretched.wav\"\n",
    "        if not tts_wav.exists():\n",
    "            print(f\"[skip] No TTS audio for seg {idx}\")\n",
    "            continue\n",
    "\n",
    "        if speaker not in ref_map:\n",
    "            print(f\"[skip] No reference for {speaker}\")\n",
    "            continue\n",
    "\n",
    "        tgt_se = torch.load(ref_map[speaker][\"embedding\"])\n",
    "        out_wav = out_dir / f\"converted_seg_{idx}_{speaker}.wav\"\n",
    "\n",
    "        tone_color_converter.convert(\n",
    "            audio_src_path=str(tts_wav),\n",
    "            src_se=src_se,   # <-- reuse the same source embedding\n",
    "            tgt_se=tgt_se,\n",
    "            output_path=str(out_wav),\n",
    "            message=\"@MyShell\",\n",
    "        )\n",
    "        print(f\"[ok] Converted seg {idx} ({speaker}) -> {out_wav}\")\n",
    "        results[idx] = str(out_wav)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio too short, fail to add watermark\n",
      "[ok] Converted seg 0 (Speaker_1) -> converted_segments/converted_seg_0_Speaker_1.wav\n",
      "[ok] Converted seg 1 (Speaker_1) -> converted_segments/converted_seg_1_Speaker_1.wav\n",
      "[ok] Converted seg 2 (Speaker_1) -> converted_segments/converted_seg_2_Speaker_1.wav\n",
      "[ok] Converted seg 3 (Speaker_2) -> converted_segments/converted_seg_3_Speaker_2.wav\n",
      "Audio too short, fail to add watermark\n",
      "[ok] Converted seg 4 (Speaker_2) -> converted_segments/converted_seg_4_Speaker_2.wav\n",
      "[ok] Converted seg 5 (Speaker_1) -> converted_segments/converted_seg_5_Speaker_1.wav\n",
      "[ok] Converted seg 6 (Speaker_2) -> converted_segments/converted_seg_6_Speaker_2.wav\n",
      "Audio too short, fail to add watermark\n",
      "[ok] Converted seg 7 (Speaker_3) -> converted_segments/converted_seg_7_Speaker_3.wav\n",
      "[ok] Converted seg 8 (Speaker_3) -> converted_segments/converted_seg_8_Speaker_3.wav\n"
     ]
    }
   ],
   "source": [
    "converted = convert_tts_segments_with_refs(\n",
    "    tts_dir=\"adjusted_segments\",\n",
    "    gemini_data=gemini_data,\n",
    "    ref_map=ref_map,\n",
    "    out_dir=\"converted_segments\",\n",
    "    src_se=src_se,\n",
    "    tone_color_converter=tone_color_converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlayed segment 0 (Speaker 1) at 0 ms\n",
      "Overlayed segment 1 (Speaker 1) at 1810 ms\n",
      "Overlayed segment 2 (Speaker 1) at 4850 ms\n",
      "Overlayed segment 3 (Speaker 2) at 9980 ms\n",
      "Overlayed segment 4 (Speaker 2) at 15220 ms\n",
      "Overlayed segment 5 (Speaker 1) at 16290 ms\n",
      "Overlayed segment 6 (Speaker 2) at 19460 ms\n",
      "Overlayed segment 7 (Speaker 3) at 26310 ms\n",
      "Overlayed segment 8 (Speaker 3) at 28610 ms\n",
      "Final audio mix saved to final_mix.wav\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def mix_segments_with_background(gemini_json_path, converted_dir, background_audio_path, output_path):\n",
    "    # Load segment metadata\n",
    "    with open(gemini_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gemini_data = json.load(f)\n",
    "\n",
    "    speak_segments = gemini_data[\"speak_segments\"]\n",
    "\n",
    "    # Load background audio\n",
    "    bg_audio = AudioSegment.from_file(background_audio_path, format=\"mp3\")\n",
    "    \n",
    "    # Overlay each converted segment\n",
    "    for idx, seg in enumerate(speak_segments):\n",
    "        speaker = seg[\"speaker\"].replace(' ', '_')\n",
    "        seg_file = Path(converted_dir) / f\"converted_seg_{idx}_{speaker}.wav\"\n",
    "        if not seg_file.exists():\n",
    "            print(f\"Skipping missing segment: {seg_file}\")\n",
    "            continue\n",
    "        \n",
    "        segment_audio = AudioSegment.from_file(seg_file, format=\"wav\")\n",
    "        start_ms = int(float(seg[\"start_time\"]) * 1000)\n",
    "        bg_audio = bg_audio.overlay(segment_audio, position=start_ms)\n",
    "        print(f\"Overlayed segment {idx} ({seg['speaker']}) at {start_ms} ms\")\n",
    "\n",
    "    # Export final mix\n",
    "    bg_audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Final audio mix saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "mix_segments_with_background(\n",
    "    gemini_json_path=\"translated_diarized_data.json\",\n",
    "    converted_dir=\"converted_segments\",\n",
    "    background_audio_path=\"separated_audio/htdemucs/media_1_audio/no_vocals.wav\",\n",
    "    output_path=\"final_mix.wav\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video output_video_with_audio.mp4.\n",
      "MoviePy - Writing audio in output_video_with_audioTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video output_video_with_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready output_video_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Load video and audio clips\n",
    "video = VideoFileClip(\"media_1_video.mp4\")\n",
    "audio = AudioFileClip(\"final_mix.wav\")\n",
    "\n",
    "# Set the new audio to the video\n",
    "final_video = video.with_audio(audio)\n",
    "\n",
    "# Export the combined file\n",
    "final_video.write_videofile(\"output_video_with_audio.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moviepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
