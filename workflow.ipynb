{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Step 1: Extract Video & Audio from MP4  \n",
    "\n",
    "In this step, we **separate the video and audio** from an MP4 file using **FFmpeg** inside a specific Conda environment.  \n",
    "\n",
    "We perform two operations:  \n",
    "- **Extract video only (no audio)**\n",
    "- **Extract audio only (no video)**  \n",
    "\n",
    "To ensure the correct FFmpeg version is used, we run it via `conda run` from the specified Conda environment.\n",
    "\n",
    "âœ… **Input:**  \n",
    "- `video1.mp4` (original video with audio)\n",
    "\n",
    "âœ… **Output:**  \n",
    "- `media_1_video.mp4` (only video, no audio)  \n",
    "- `media_1_audio.mp3` (only audio, no video)  \n",
    "\n",
    "Run the following code to perform the extraction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define your Conda environment name\n",
    "CONDA_ENV = \"openvoice\"  # Change this to your actual Conda environment name\n",
    "\n",
    "def extract_video(input_file, output_video):\n",
    "    command = [\"conda\", \"run\", \"-n\", CONDA_ENV, \"ffmpeg\", \"-i\", input_file, \"-c:v\", \"copy\", \"-an\", output_video]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "def extract_audio(input_file, output_audio):\n",
    "    command = [\"conda\", \"run\", \"-n\", CONDA_ENV, \"ffmpeg\", \"-i\", input_file, \"-q:a\", \"0\", \"-map\", \"a\", output_audio]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "# Example Usage\n",
    "input_file = \"video1.mp4\"\n",
    "output_video = \"media_1_video.mp4\"\n",
    "output_audio = \"media_1_audio.mp3\"\n",
    "\n",
    "extract_video(input_file, output_video)\n",
    "extract_audio(input_file, output_audio)\n",
    "\n",
    "print(\"Video and Audio extracted successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Step 2: Separate Vocals & Background with Demucs (Using Conda)\n",
    "We use **Demucs** to separate vocals and background music, ensuring it runs inside the correct **Conda environment**.\n",
    "\n",
    "### âœ… **Process:**\n",
    "- **Input:** `output_audio.mp3` (extracted from video)\n",
    "- **Output:**\n",
    "  - `vocals.wav` â†’ ðŸŽ¤ **Vocals only**\n",
    "  - `no_vocals.wav` â†’ ðŸŽ¼ **Music only**\n",
    "\n",
    "### ðŸ”¹ **Command Used (Runs in `openvoice` Conda Env)**\n",
    "```bash\n",
    "conda run -n openvoice demucs --two-stems vocals --out separated_audio output_audio.mp3\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mImportant: the default model was recently changed to `htdemucs`\u001b[0m the latest Hybrid Transformer Demucs model. In some cases, this model can actually perform worse than previous models. To get back the old default model use `-n mdx_extra_q`.\n",
      "Selected model is a bag of 1 models. You will see that many progress bars per track.\n",
      "Separated tracks will be stored in /mnt/d/dev/tataplay/separated_audio/htdemucs\n",
      "Separating track media_1_audio.mp3\n",
      "\n",
      "âœ… Separation complete! Check the 'separated_audio' folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                     | 0.0/40.949999999999996 [00:00<?, ?seconds/s]\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                   | 5.85/40.949999999999996 [00:03<00:19,  1.79seconds/s]\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 11.7/40.949999999999996 [00:05<00:12,  2.33seconds/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 17.549999999999997/40.949999999999996 [00:07<00:08,  2.61seconds/s]\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 23.4/40.949999999999996 [00:09<00:06,  2.79seconds/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 29.25/40.949999999999996 [00:10<00:04,  2.87seconds/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 35.099999999999994/40.949999999999996 [00:12<00:01,  2.94seconds/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40.949999999999996/40.949999999999996 [00:14<00:00,  2.96seconds/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40.949999999999996/40.949999999999996 [00:14<00:00,  2.76seconds/s]\n",
      "/home/skamalj/miniconda3/envs/openvoice/lib/python3.9/site-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def separate_audio_with_demucs_conda(input_audio, output_dir=\"separated_audio\", conda_env=\"openvoice\"):\n",
    "    \"\"\"\n",
    "    Runs Demucs using subprocess via conda run to ensure it executes in the correct environment.\n",
    "\n",
    "    Args:\n",
    "        input_audio (str): Path to input audio file (MP3/WAV).\n",
    "        output_dir (str): Directory to save separated audio.\n",
    "        conda_env (str): Name of the Conda environment with Demucs installed.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Command to run Demucs inside the specified Conda environment\n",
    "        command = [\n",
    "            \"conda\", \"run\", \"-n\", conda_env,  # Run in the Conda environment\n",
    "            \"demucs\", \"--two-stems\", \"vocals\",  # Extract vocals & background\n",
    "            \"--out\", output_dir,  # Save to output directory\n",
    "            input_audio  # Input audio file\n",
    "        ]\n",
    "\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"âœ… Separation complete! Check the '{output_dir}' folder.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "separate_audio_with_demucs_conda(\"media_1_audio.mp3\", conda_env=\"openvoice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Audio file path\n",
    "#audio_path = \"separated_audio/htdemucs/media_1_audio/vocals.wav\"\n",
    "audio_path = \"media_1_audio.mp3\"\n",
    "client = OpenAI()\n",
    "# Open the audio file for transcription\n",
    "with open(audio_path, \"rb\") as audio_file:\n",
    "    response =  client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",  # OpenAI Whisper API model\n",
    "        file=audio_file,\n",
    "        response_format=\"verbose_json\",  # Ensure JSON output\n",
    "        timestamp_granularities=[\"word\", \"segment\"]  # Enable detailed timestamps\n",
    "    )\n",
    "\n",
    "print(response)\n",
    "# Extract useful data in JSON format\n",
    "transcription_data = {\n",
    "    \"text\": response.text,  # Full transcription\n",
    "    \"language\": getattr(response, \"language\", \"unknown\"),  # Detected language\n",
    "    \"segments\": []  # List of segments with timestamps\n",
    "}\n",
    "\n",
    "# Preserve detailed segment & word-level timestamps\n",
    "if hasattr(response, \"segments\"):\n",
    "    for segment in response.segments:\n",
    "        transcription_data[\"segments\"].append({\n",
    "            \"start\": segment.start,  # Accessing attribute with dot notation\n",
    "            \"end\": segment.end,      # Accessing attribute with dot notation\n",
    "            \"text\": segment.text,    # Accessing attribute with dot notation\n",
    "            \"confidence\": getattr(segment, \"confidence\", 1.0),  # Default confidence\n",
    "        })\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"transcription.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(transcription_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Transcription saved to transcription.json (Detected Language: {response.language})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyannote.audio import Pipeline\n",
    "import os\n",
    "\n",
    "# Load the pretrained speaker diarization model\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=os.getenv(\"HUGGINGFACE_AUTH_TOKEN_TATA\"))\n",
    "\n",
    "# Define input audio file\n",
    "audio_file = \"media_1_audio.mp3\"\n",
    "\n",
    "# Run diarization **once** and store in a variable\n",
    "diarization_result = list(pipeline(audio_file).itertracks(yield_label=True))\n",
    "print(diarization_result)\n",
    "# Load existing Whisper transcription\n",
    "with open(\"transcription.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    transcription_data = json.load(f)\n",
    "\n",
    "# Assign speakers to transcription segments using stored diarization results\n",
    "for segment in transcription_data[\"segments\"]:\n",
    "    segment_start, segment_end = segment[\"start\"], segment[\"end\"]\n",
    "    \n",
    "    for turn, _, speaker in diarization_result:\n",
    "        if turn.start <= segment_start <= turn.end or turn.start <= segment_end <= turn.end:\n",
    "            segment[\"speaker_id\"] = speaker\n",
    "            break  # Stop searching once a match is found\n",
    "\n",
    "# Save updated transcription with speaker IDs\n",
    "with open(\"transcription.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(transcription_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… Updated transcription.json with speaker IDs!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvoice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
