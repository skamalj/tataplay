{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Step 1: Extract Video & Audio from MP4  \n",
    "\n",
    "In this step, we **separate the video and audio** from an MP4 file using **FFmpeg** inside a specific Conda environment.  \n",
    "\n",
    "We perform two operations:  \n",
    "- **Extract video only (no audio)**\n",
    "- **Extract audio only (no video)**  \n",
    "\n",
    "To ensure the correct FFmpeg version is used, we run it via `conda run` from the specified Conda environment.\n",
    "\n",
    "✅ **Input:**  \n",
    "- `video1.mp4` (original video with audio)\n",
    "\n",
    "✅ **Output:**  \n",
    "- `media_1_video.mp4` (only video, no audio)  \n",
    "- `media_1_audio.mp3` (only audio, no video)  \n",
    "\n",
    "Run the following code to perform the extraction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define your Conda environment name\n",
    "CONDA_ENV = \"tataplay\"  # Change this to your actual Conda environment name\n",
    "\n",
    "def extract_video(input_file, output_video):\n",
    "    command = [\"conda\", \"run\", \"-n\", CONDA_ENV, \"ffmpeg\", \"-i\", input_file, \"-c:v\", \"copy\", \"-an\", output_video]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "def extract_audio(input_file, output_audio):\n",
    "    command = [\"conda\", \"run\", \"-n\", CONDA_ENV, \"ffmpeg\", \"-i\", input_file, \"-q:a\", \"0\", \"-map\", \"a\", output_audio]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "# Example Usage\n",
    "input_file = \"video1.mp4\"\n",
    "output_video = \"media_1_video.mp4\"\n",
    "output_audio = \"media_1_audio.mp3\"\n",
    "\n",
    "extract_video(input_file, output_video)\n",
    "extract_audio(input_file, output_audio)\n",
    "\n",
    "print(\"Video and Audio extracted successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 Step 2: Separate Vocals & Background with Demucs (Using Conda)\n",
    "We use **Demucs** to separate vocals and background music, ensuring it runs inside the correct **Conda environment**.\n",
    "\n",
    "### ✅ **Process:**\n",
    "- **Input:** `output_audio.mp3` (extracted from video)\n",
    "- **Output:**\n",
    "  - `vocals.wav` → 🎤 **Vocals only**\n",
    "  - `no_vocals.wav` → 🎼 **Music only**\n",
    "\n",
    "### 🔹 **Command Used (Runs in `openvoice` Conda Env)**\n",
    "```bash\n",
    "conda run -n openvoice demucs --two-stems vocals --out separated_audio output_audio.mp3\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mImportant: the default model was recently changed to `htdemucs`\u001b[0m the latest Hybrid Transformer Demucs model. In some cases, this model can actually perform worse than previous models. To get back the old default model use `-n mdx_extra_q`.\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/demucs/hybrid_transformer/955717e8-8726e21a.th\" to /home/skamalj/.cache/torch/hub/checkpoints/955717e8-8726e21a.th\n",
      "Selected model is a bag of 1 models. You will see that many progress bars per track.\n",
      "Separated tracks will be stored in /mnt/d/dev/tataplay/separated_audio/htdemucs\n",
      "Separating track media_1_audio.mp3\n",
      "\n",
      "✅ Separation complete! Check the 'separated_audio' folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.2M [00:00<?, ?B/s]\n",
      "  2%|▏         | 1.75M/80.2M [00:00<00:04, 16.5MB/s]\n",
      "  6%|▌         | 4.50M/80.2M [00:00<00:03, 23.4MB/s]\n",
      "  9%|▊         | 6.88M/80.2M [00:00<00:03, 23.4MB/s]\n",
      " 12%|█▏        | 9.62M/80.2M [00:00<00:02, 25.0MB/s]\n",
      " 16%|█▌        | 12.9M/80.2M [00:00<00:02, 24.9MB/s]\n",
      " 19%|█▉        | 15.5M/80.2M [00:00<00:02, 25.4MB/s]\n",
      " 24%|██▍       | 19.6M/80.2M [00:00<00:02, 30.7MB/s]\n",
      " 28%|██▊       | 22.6M/80.2M [00:00<00:02, 29.5MB/s]\n",
      " 32%|███▏      | 25.5M/80.2M [00:01<00:04, 14.1MB/s]\n",
      " 35%|███▍      | 27.8M/80.2M [00:01<00:05, 9.93MB/s]\n",
      " 37%|███▋      | 29.5M/80.2M [00:02<00:06, 8.70MB/s]\n",
      " 38%|███▊      | 30.9M/80.2M [00:02<00:06, 8.58MB/s]\n",
      " 40%|████      | 32.1M/80.2M [00:02<00:06, 8.20MB/s]\n",
      " 42%|████▏     | 33.4M/80.2M [00:02<00:05, 8.61MB/s]\n",
      " 43%|████▎     | 34.4M/80.2M [00:02<00:05, 8.61MB/s]\n",
      " 44%|████▍     | 35.4M/80.2M [00:02<00:05, 8.67MB/s]\n",
      " 45%|��███▌     | 36.4M/80.2M [00:02<00:05, 8.96MB/s]\n",
      " 47%|████▋     | 37.4M/80.2M [00:03<00:05, 7.78MB/s]\n",
      " 48%|████▊     | 38.2M/80.2M [00:03<00:06, 6.73MB/s]\n",
      " 49%|████▊     | 39.0M/80.2M [00:03<00:06, 6.23MB/s]\n",
      " 50%|████▉     | 39.8M/80.2M [00:03<00:07, 5.57MB/s]\n",
      " 50%|█████     | 40.4M/80.2M [00:03<00:07, 5.69MB/s]\n",
      " 51%|█████     | 41.1M/80.2M [00:03<00:06, 6.04MB/s]\n",
      " 52%|█████▏    | 42.0M/80.2M [00:04<00:06, 6.49MB/s]\n",
      " 54%|█████▎    | 43.1M/80.2M [00:04<00:05, 7.24MB/s]\n",
      " 55%|█████▌    | 44.2M/80.2M [00:04<00:04, 8.14MB/s]\n",
      " 56%|█████▌    | 45.1M/80.2M [00:04<00:04, 8.12MB/s]\n",
      " 57%|█████▋    | 46.0M/80.2M [00:04<00:04, 8.12MB/s]\n",
      " 58%|█████▊    | 46.9M/80.2M [00:04<00:05, 6.66MB/s]\n",
      " 60%|█████▉    | 47.9M/80.2M [00:04<00:04, 7.37MB/s]\n",
      " 61%|██████    | 48.6M/80.2M [00:04<00:04, 7.39MB/s]\n",
      " 62%|██████▏   | 49.5M/80.2M [00:05<00:04, 7.64MB/s]\n",
      " 63%|██████▎   | 50.6M/80.2M [00:05<00:03, 8.71MB/s]\n",
      " 64%|██████▍   | 51.6M/80.2M [00:05<00:03, 9.09MB/s]\n",
      " 66%|██████▌   | 52.6M/80.2M [00:05<00:03, 8.04MB/s]\n",
      " 67%|██████▋   | 53.5M/80.2M [00:05<00:03, 7.80MB/s]\n",
      " 68%|██████▊   | 54.4M/80.2M [00:05<00:03, 7.04MB/s]\n",
      " 69%|██████▊   | 55.1M/80.2M [00:05<00:03, 7.06MB/s]\n",
      " 70%|██████▉   | 55.9M/80.2M [00:05<00:03, 6.80MB/s]\n",
      " 71%|███████   | 56.6M/80.2M [00:06<00:03, 7.02MB/s]\n",
      " 72%|███████▏  | 57.4M/80.2M [00:06<00:03, 7.05MB/s]\n",
      " 73%|███████▎  | 58.2M/80.2M [00:06<00:03, 7.50MB/s]\n",
      " 74%|███████▍  | 59.2M/80.2M [00:06<00:02, 8.28MB/s]\n",
      " 75%|███████▍  | 60.1M/80.2M [00:06<00:02, 8.09MB/s]\n",
      " 76%|███████▌  | 61.0M/80.2M [00:06<00:02, 7.73MB/s]\n",
      " 77%|███████▋  | 61.8M/80.2M [00:06<00:03, 6.38MB/s]\n",
      " 78%|███████▊  | 62.5M/80.2M [00:06<00:03, 5.92MB/s]\n",
      " 79%|███████▊  | 63.1M/80.2M [00:07<00:03, 5.78MB/s]\n",
      " 80%|████████  | 64.2M/80.2M [00:07<00:02, 7.10MB/s]\n",
      " 81%|████████  | 65.0M/80.2M [00:07<00:02, 7.07MB/s]\n",
      " 82%|████████▏ | 65.8M/80.2M [00:07<00:02, 7.02MB/s]\n",
      " 83%|████████▎ | 66.5M/80.2M [00:07<00:02, 7.11MB/s]\n",
      " 84%|████████▍ | 67.6M/80.2M [00:07<00:01, 8.10MB/s]\n",
      " 86%|████████▌ | 68.6M/80.2M [00:07<00:01, 8.63MB/s]\n",
      " 87%|████████▋ | 69.6M/80.2M [00:07<00:01, 9.06MB/s]\n",
      " 88%|████████▊ | 70.5M/80.2M [00:07<00:01, 7.24MB/s]\n",
      " 89%|████████▉ | 71.5M/80.2M [00:08<00:01, 7.85MB/s]\n",
      " 91%|█████████ | 72.6M/80.2M [00:08<00:00, 8.74MB/s]\n",
      " 92%|█████████▏| 73.8M/80.2M [00:08<00:00, 9.38MB/s]\n",
      " 93%|█████████▎| 74.9M/80.2M [00:08<00:00, 9.72MB/s]\n",
      " 95%|█████████▍| 75.9M/80.2M [00:08<00:00, 8.46MB/s]\n",
      " 96%|█████████▌| 77.1M/80.2M [00:08<00:00, 9.58MB/s]\n",
      " 97%|█████████▋| 78.1M/80.2M [00:08<00:00, 9.19MB/s]\n",
      " 99%|█████████▊| 79.1M/80.2M [00:08<00:00, 8.16MB/s]\n",
      "100%|█████████▉| 80.0M/80.2M [00:09<00:00, 7.93MB/s]\n",
      "100%|██████████| 80.2M/80.2M [00:09<00:00, 9.23MB/s]\n",
      "\n",
      "  0%|                                                                     | 0.0/40.949999999999996 [00:00<?, ?seconds/s]\n",
      " 14%|████████▌                                                   | 5.85/40.949999999999996 [00:05<00:31,  1.11seconds/s]\n",
      " 29%|█████████████████▏                                          | 11.7/40.949999999999996 [00:09<00:22,  1.30seconds/s]\n",
      " 43%|███████████████████▋                          | 17.549999999999997/40.949999999999996 [00:13<00:17,  1.36seconds/s]\n",
      " 57%|█████��████████████████████████████▎                         | 23.4/40.949999999999996 [00:17<00:12,  1.38seconds/s]\n",
      " 71%|██████████████████████████████████████████▏                | 29.25/40.949999999999996 [00:21<00:08,  1.39seconds/s]\n",
      " 86%|███████████████████████████████████████▍      | 35.099999999999994/40.949999999999996 [00:25<00:04,  1.38seconds/s]\n",
      "100%|██████████████████████████████████████████████| 40.949999999999996/40.949999999999996 [00:30<00:00,  1.37seconds/s]\n",
      "100%|██████████████████████████████████████████████| 40.949999999999996/40.949999999999996 [00:30<00:00,  1.36seconds/s]\n",
      "/home/skamalj/.conda/envs/tataplay/lib/python3.11/site-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "/home/skamalj/.conda/envs/tataplay/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def separate_audio_with_demucs_conda(input_audio, output_dir=\"separated_audio\", conda_env=\"openvoice\"):\n",
    "    \"\"\"\n",
    "    Runs Demucs using subprocess via conda run to ensure it executes in the correct environment.\n",
    "\n",
    "    Args:\n",
    "        input_audio (str): Path to input audio file (MP3/WAV).\n",
    "        output_dir (str): Directory to save separated audio.\n",
    "        conda_env (str): Name of the Conda environment with Demucs installed.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Command to run Demucs inside the specified Conda environment\n",
    "        command = [\n",
    "            \"conda\", \"run\", \"-n\", conda_env,  # Run in the Conda environment\n",
    "            \"demucs\", \"--two-stems\", \"vocals\",  # Extract vocals & background\n",
    "            \"--out\", output_dir,  # Save to output directory\n",
    "            input_audio  # Input audio file\n",
    "        ]\n",
    "\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"✅ Separation complete! Check the '{output_dir}' folder.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "separate_audio_with_demucs_conda(\"media_1_audio.mp3\", conda_env=\"tataplay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Audio file path\n",
    "#audio_path = \"separated_audio/htdemucs/media_1_audio/vocals.wav\"\n",
    "audio_path = \"media_1_audio.mp3\"\n",
    "client = OpenAI()\n",
    "# Open the audio file for transcription\n",
    "with open(audio_path, \"rb\") as audio_file:\n",
    "    response =  client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",  # OpenAI Whisper API model\n",
    "        file=audio_file,\n",
    "        response_format=\"verbose_json\",  # Ensure JSON output\n",
    "        timestamp_granularities=[\"word\", \"segment\"]  # Enable detailed timestamps\n",
    "    )\n",
    "\n",
    "print(response)\n",
    "# Extract useful data in JSON format\n",
    "transcription_data = {\n",
    "    \"text\": response.text,  # Full transcription\n",
    "    \"language\": getattr(response, \"language\", \"unknown\"),  # Detected language\n",
    "    \"segments\": []  # List of segments with timestamps\n",
    "}\n",
    "\n",
    "# Preserve detailed segment & word-level timestamps\n",
    "if hasattr(response, \"segments\"):\n",
    "    for segment in response.segments:\n",
    "        transcription_data[\"segments\"].append({\n",
    "            \"start\": segment.start,  # Accessing attribute with dot notation\n",
    "            \"end\": segment.end,      # Accessing attribute with dot notation\n",
    "            \"text\": segment.text,    # Accessing attribute with dot notation\n",
    "            \"confidence\": getattr(segment, \"confidence\", 1.0),  # Default confidence\n",
    "        })\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"transcription.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(transcription_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Transcription saved to transcription.json (Detected Language: {response.language})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyannote.audio import Pipeline\n",
    "import os\n",
    "\n",
    "# Load the pretrained speaker diarization model\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=os.getenv(\"HUGGINGFACE_AUTH_TOKEN_TATA\"))\n",
    "\n",
    "# Define input audio file\n",
    "audio_file = \"media_1_audio.mp3\"\n",
    "\n",
    "# Run diarization **once** and store in a variable\n",
    "diarization_result = list(pipeline(audio_file).itertracks(yield_label=True))\n",
    "print(diarization_result)\n",
    "# Load existing Whisper transcription\n",
    "with open(\"transcription.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    transcription_data = json.load(f)\n",
    "\n",
    "# Assign speakers to transcription segments using stored diarization results\n",
    "for segment in transcription_data[\"segments\"]:\n",
    "    segment_start, segment_end = segment[\"start\"], segment[\"end\"]\n",
    "    \n",
    "    for turn, _, speaker in diarization_result:\n",
    "        if turn.start <= segment_start <= turn.end or turn.start <= segment_end <= turn.end:\n",
    "            segment[\"speaker_id\"] = speaker\n",
    "            break  # Stop searching once a match is found\n",
    "\n",
    "# Save updated transcription with speaker IDs\n",
    "with open(\"transcription.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(transcription_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Updated transcription.json with speaker IDs!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
